# Glosario: Gradiente
# Â¿QuÃ© es un gradiente?

El gradiente mide cÃ³mo cambia una funciÃ³n respecto a sus variables de entrada.
En aprendizaje automÃ¡tico, representa la direcciÃ³n y magnitud del cambio que debe hacerse en los pesos del modelo para minimizar el error (funciÃ³n de pÃ©rdida).

En palabras simples:

El gradiente te dice cuÃ¡nto y en quÃ© direcciÃ³n ajustar cada parÃ¡metro del modelo para mejorar su desempeÃ±o.

## 1. DefiniciÃ³n matemÃ¡tica

Si tenemos una funciÃ³n ğ‘“(ğ‘¥), su gradiente es la derivada respecto a ğ‘¥:

dx / dfâ€‹

Si la funciÃ³n depende de varios parÃ¡metros (x1â€‹,x2â€‹,x3â€‹,â€¦,xnâ€‹), el gradiente es un vector de derivadas parciales:

âˆ‡f(x)=(âˆ‚x1â€‹âˆ‚fâ€‹,âˆ‚x2â€‹âˆ‚fâ€‹,â€¦,âˆ‚xnâ€‹âˆ‚fâ€‹)

En el contexto de redes neuronales:

f(x) = funciÃ³n de pÃ©rdida (error)
x sub i = pesos y sesgos del modelo
	â€‹ 
## 2. Gradiente en PyTorch
## 3. Gradiente descendente (Gradient Descent)
## 4. PropagaciÃ³n hacia atrÃ¡s (Backpropagation)
## 5. ExplosiÃ³n y desapariciÃ³n de gradientes
## 6. Gradiente y requires_grad
## 7. .grad, .backward() y .detach()
## 8. VisualizaciÃ³n intuitiva

En resumen

| Concepto                   | Significado                                                |
| -------------------------- | ---------------------------------------------------------- |
| **Gradiente**              | Derivada que indica direcciÃ³n y magnitud del cambio Ã³ptimo |
| **Autograd**               | Sistema automÃ¡tico de cÃ¡lculo de gradientes de PyTorch     |
| **Backpropagation**        | PropagaciÃ³n inversa del error para ajustar pesos           |
| **Gradient Descent**       | Algoritmo que usa gradientes para minimizar la pÃ©rdida     |
| **ExplosiÃ³n/DesapariciÃ³n** | Problemas por gradientes demasiado grandes o pequeÃ±os      |

