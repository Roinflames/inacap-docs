# Glosario: Pesos (Weights)
# Â¿QuÃ© son los pesos?
Los pesos son los parÃ¡metros ajustables de una red neuronal.
Determinan cuÃ¡nto influye cada entrada en la salida de una neurona o capa.

ğŸ“– En palabras simples:

Los pesos son los â€œbotonesâ€ que la red ajusta durante el entrenamiento para aprender relaciones entre los datos.
## 1. RepresentaciÃ³n matemÃ¡tica
En una red neuronal simple (perceptrÃ³n):
```
y=f(w1â€‹x1â€‹+w2â€‹x2â€‹+â‹¯+wnâ€‹xnâ€‹+b)
```
Donde:
ğ‘¥ğ‘– â†’ entradas
ğ‘¤ğ‘– â†’ pesos
ğ‘ â†’ sesgo (bias)
ğ‘“ â†’ funciÃ³n de activaciÃ³n
ğ‘¦ â†’ salida

Los pesos determinan la pendiente o inclinaciÃ³n de la funciÃ³n.
El entrenamiento busca valores de ğ‘¤ğ‘– y ğ‘ que minimicen el error (loss).
## 2. Pesos en PyTorch
En PyTorch, los pesos estÃ¡n almacenados dentro de los mÃ³dulos (nn.Module) y son tensores con gradiente habilitado.
```python
import torch
import torch.nn as nn

# Red simple
modelo = nn.Linear(2, 1)  # 2 entradas, 1 salida

print(modelo.weight)  # Pesos
print(modelo.bias)    # Bias
```
Salida tÃ­pica:
```bash
Parameter containing:
tensor([[0.1234, -0.5321]], requires_grad=True)
```
Nota: requires_grad=True significa que PyTorch calcularÃ¡ gradientes sobre esos pesos durante el entrenamiento.
## 3. InicializaciÃ³n de pesos
Antes de entrenar, los pesos se inicializan (normalmente al azar).
Esto es importante porque una mala inicializaciÃ³n puede impedir el aprendizaje.

Ejemplos comunes:
- Uniforme o normal aleatoria: valores pequeÃ±os al azar.
- Xavier/Glorot: ideal para capas lineales con activaciones simÃ©tricas (tanh).
- He initialization: ideal para ReLU o LeakyReLU.

```python
nn.init.xavier_uniform_(modelo.weight)
nn.init.zeros_(modelo.bias)
```
## 4. ActualizaciÃ³n de pesos
Durante el entrenamiento, los pesos se actualizan usando el gradiente descendente:
wnuevo â€‹= wviejoâ€‹ âˆ’ Î· â‹… (âˆ‚L/âˆ‚wâ€‹)

Donde:

- ğœ‚: tasa de aprendizaje (learning rate)
- âˆ‚ğ¿/âˆ‚ğ‘¤: gradiente del error respecto al peso

```python
optimizer.zero_grad()  # Limpia gradientes
loss.backward()        # Calcula gradientes
optimizer.step()       # Actualiza pesos
```
## 5. Bias (sesgo)
El bias o sesgo es un parÃ¡metro adicional que permite desplazar la funciÃ³n de activaciÃ³n,
lo que ayuda a que el modelo aprenda relaciones incluso si las entradas son cero.

Ejemplo en PyTorch:
```python
print(modelo.bias)  # tensor con requires_grad=True
```
## 6. Congelar o desbloquear pesos
A veces se necesita congelar pesos (por ejemplo, al hacer transfer learning).
AsÃ­ PyTorch no los actualiza durante el entrenamiento:
```python
for param in modelo.parameters():
    param.requires_grad = False
```
Para volver a entrenarlos:
```python
for param in modelo.parameters():
    param.requires_grad = True
```
## 7. VisualizaciÃ³n e inspecciÃ³n de pesos
Puedes ver los valores actuales de los pesos de un modelo:
```python
for name, param in modelo.named_parameters():
    print(name, param.data)
```
O convertirlos a NumPy:
```python
param_numpy = modelo.weight.data.numpy()
```
## 8. Pesos en distintas capas
| Tipo de capa               | Nombre de parÃ¡metro            | Forma tÃ­pica      | Ejemplo                                |
| -------------------------- | ------------------------------ | ----------------- | -------------------------------------- |
| `nn.Linear(in, out)`       | `weight`                       | `[out, in]`       | `[10, 5]` â†’ 10 neuronas, 5 entradas    |
| `nn.Conv2d(in, out, k, k)` | `weight`                       | `[out, in, k, k]` | 32 filtros de 3Ã—3 sobre 3 canales      |
| `nn.LSTM`                  | `weight_ih_l0`, `weight_hh_l0` | Matrices grandes  | Para conexiones internas y recurrentes |

## 9. RegularizaciÃ³n de pesos
Para evitar sobreajuste, se aplican penalizaciones a los pesos grandes:

L2 regularization (Weight decay):

```text
L' = L + Î» âˆ‘(wiÂ²)
```

En PyTorch:

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
```

## 10. Guardar y cargar pesos
Los pesos son lo que realmente â€œaprendeâ€ un modelo.
Puedes guardarlos y reutilizarlos fÃ¡cilmente:

```python
torch.save(modelo.state_dict(), 'pesos.pth')
modelo.load_state_dict(torch.load('pesos.pth'))
```

## En resumen
| Concepto            | DescripciÃ³n                                                        |
| ------------------- | ------------------------------------------------------------------ |
| **Pesos (Weights)** | ParÃ¡metros ajustables que determinan la influencia de cada entrada |
| **Bias (sesgo)**    | Desplaza la funciÃ³n de activaciÃ³n                                  |
| **InicializaciÃ³n**  | Valores iniciales de los pesos (aleatorios o estratÃ©gicos)         |
| **ActualizaciÃ³n**   | Se modifican con gradiente descendente                             |
| **RegularizaciÃ³n**  | Evita pesos excesivos para mejorar generalizaciÃ³n                  |
| **State dict**      | Estructura donde PyTorch guarda los pesos y biases                 |

# Ejemplo visual (mental):

Entrada â†’ [ w1, w2, w3, ... ] â†’ Neurona â†’ ActivaciÃ³n â†’ Salida
          â†‘ Pesos que se ajustan en cada epoch
