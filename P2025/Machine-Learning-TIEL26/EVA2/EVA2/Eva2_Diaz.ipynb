{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# SHIA ENTERPRISE HYBRID: Spark + Dask + Pandas (Código completo corregido)\n",
        "# ==========================================================\n",
        "# Ejecutar en Jupyter / Colab / entorno con PySpark, Dask instalados.\n",
        "# Si falta alguna librería, descomenta la instalación siguiente (notebooks).\n",
        "# !pip install pyspark dask distributed pandas scikit-learn joblib mlflow requests --quiet\n",
        "\n",
        "# ----------------------------\n",
        "# IMPORTS\n",
        "# ----------------------------\n",
        "import os\n",
        "import requests\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Dask\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "# ML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Stacking / SHIA-level\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "\n",
        "# Repro\n",
        "RND = 42\n",
        "np.random.seed(RND)"
      ],
      "metadata": {
        "id": "NGpY_M3UCnvt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1) Descargar CSV localmente (Spark no lee https directamente)\n",
        "# ----------------------------\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "local_path = \"/tmp/titanic.csv\"\n",
        "os.makedirs(\"/tmp\", exist_ok=True)\n",
        "\n",
        "if not os.path.exists(local_path):\n",
        "    print(\"Descargando dataset:\", url)\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "    print(\"Descarga completa ->\", local_path)\n",
        "else:\n",
        "    print(\"Archivo ya existe:\", local_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJwedBCBCqpD",
        "outputId": "dcf62936-74d6-4437-bcc8-c4be7a382299"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo ya existe: /tmp/titanic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2) Cargar con Pandas (inspección rápida)\n",
        "# ----------------------------\n",
        "pdf = pd.read_csv(local_path)\n",
        "print(\"Pandas df shape:\", pdf.shape)\n",
        "print(pdf.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6yuT5NzCsey",
        "outputId": "740fea31-f17e-497b-b1ba-b9eff37624b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas df shape: (891, 12)\n",
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3) Inicializar Spark\n",
        "# ----------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SHIA_Enterprise_Hybrid\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "print(\"Spark versión:\", spark.version)\n",
        "\n",
        "# Convertir Pandas -> Spark DataFrame\n",
        "spark_df = spark.createDataFrame(pdf)\n",
        "print(\"Spark DF creado:\")\n",
        "spark_df.show(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyVysZBHCuLI",
        "outputId": "2fd9682d-059a-4de7-a8bd-a245f4006aae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark versión: 3.5.1\n",
            "Spark DF creado:\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|  NaN|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|  NaN|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# 4) Convertir a Dask DataFrame (opcional)\n",
        "# ----------------------------\n",
        "import dask.dataframe as dd\n",
        "ddf = dd.from_pandas(pdf, npartitions=4)\n",
        "print(\"Dask DataFrame particionado en:\", ddf.npartitions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQHkFJhIC0Da",
        "outputId": "c6a04e85-0608-470a-fd58-217d6372e7b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask DataFrame particionado en: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5) Preprocesamiento (Spark/Pandas)\n",
        "#    - Rellenar nulos, codificar sexo, seleccionar features\n",
        "# ----------------------------\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Hacemos transformaciones en Spark (para simular pipeline a escala)\n",
        "spark_df = spark_df.fillna({'Age': 30, 'Embarked': 'S'})\n",
        "spark_df = spark_df.withColumn(\"Sex\", when(col(\"Sex\") == \"male\", 1).otherwise(0))\n",
        "# Traer a Pandas para el resto del flujo (meta/SHIA)\n",
        "pdf_clean = spark_df.toPandas()\n",
        "\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "target = \"Survived\"\n",
        "\n",
        "X = pdf_clean[features].copy()\n",
        "y = pdf_clean[target].copy()\n",
        "print(\"Features shape:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt_IYDbsC2Hc",
        "outputId": "4867df91-afd9-443e-b932-90844809bfa0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (891, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 6) Train/Val/Test split (Pandas)\n",
        "# ----------------------------\n",
        "X_train, X_hold, y_train, y_hold = train_test_split(X, y, test_size=0.30, stratify=y, random_state=RND)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_hold, y_hold, test_size=0.5, stratify=y_hold, random_state=RND)\n",
        "print(\"Train/Val/Test shapes:\", X_train.shape, X_val.shape, X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3JzqUlzC3-9",
        "outputId": "67d47b88-900b-4f02-edf2-04ef3609bee8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val/Test shapes: (623, 6) (134, 6) (134, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# 7) Iniciar Dask (LocalCluster)\n",
        "# ----------------------------\n",
        "cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit=\"2GB\")\n",
        "client = Client(cluster)\n",
        "print(\"Dask dashboard:\", client.dashboard_link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY5-gb0SC5-T",
        "outputId": "2eef58bf-b29f-4f53-ec12-0659ed9482f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 35293 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:33719\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:35293/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41557'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44279'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43847'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36333'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46465 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46465\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:40864\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:41561 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41561\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:40870\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:42435 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42435\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:40872\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:39169 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:39169\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45550\n",
            "INFO:distributed.scheduler:Receive client connection: Client-fe2f31db-adc1-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask dashboard: http://127.0.0.1:35293/status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 8) Entrenamiento de modelos base en paralelo con Dask\n",
        "#    - Función que entrena y devuelve el modelo (se recoge en el driver)\n",
        "# ----------------------------\n",
        "def train_model_return(name, model, X_tr, y_tr):\n",
        "    \"\"\"\n",
        "    Entrena un modelo (pandas DataFrame/Series) y devuelve el objeto ajustado.\n",
        "    \"\"\"\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return (name, model)\n",
        "\n",
        "# modelos base\n",
        "base_models = {\n",
        "    \"M1_rf\": RandomForestClassifier(n_estimators=100, random_state=RND),\n",
        "    \"M2_gb\": GradientBoostingClassifier(random_state=RND),\n",
        "    \"M3_svc\": SVC(probability=True, kernel='rbf', C=2, random_state=RND),\n",
        "    \"M4_lr\": LogisticRegression(max_iter=500, random_state=RND)\n",
        "}\n",
        "\n",
        "# enviar tareas a Dask (los datos X_train,y_train se serializan hacia los workers)\n",
        "futures = []\n",
        "for name, model in base_models.items():\n",
        "    fut = client.submit(train_model_return, name, model, X_train, y_train)\n",
        "    futures.append(fut)\n",
        "\n",
        "# recoger resultados (modelos ajustados en driver)\n",
        "trained = client.gather(futures)\n",
        "models_trained = {name: mdl for name, mdl in trained}\n",
        "\n",
        "# Guardar localmente (driver) con joblib\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "for name, mdl in models_trained.items():\n",
        "    path = models_dir / f\"{name}.joblib\"\n",
        "    joblib.dump(mdl, path)\n",
        "    print(\"Guardado:\", path)\n",
        "\n",
        "print(\"Modelos base entrenados:\", list(models_trained.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_w73GZJDAEL",
        "outputId": "e5cb207f-8602-4a2a-9770-86b64ca460c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardado: models/M1_rf.joblib\n",
            "Guardado: models/M2_gb.joblib\n",
            "Guardado: models/M3_svc.joblib\n",
            "Guardado: models/M4_lr.joblib\n",
            "Modelos base entrenados: ['M1_rf', 'M2_gb', 'M3_svc', 'M4_lr']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 10) Entrenar meta-learners H1, H2 y blender final Hf (Pandas/scikit-learn)\n",
        "# ----------------------------\n",
        "# dividir meta-features en dos para H1/H2\n",
        "mid = max(1, meta_val.shape[1] // 2)\n",
        "H1 = GradientBoostingClassifier(random_state=RND)\n",
        "H2 = LogisticRegression(max_iter=1000, random_state=RND)\n",
        "\n",
        "# Para entrenar H1/H2 necesitamos targets; usamos y_val aligned with meta_val rows\n",
        "H1.fit(meta_val[:, :mid], y_val)\n",
        "H2.fit(meta_val[:, mid:], y_val)\n",
        "\n",
        "# features para Hf (outputs probabilísticos de H1/H2 sobre validación)\n",
        "meta_for_Hf_val = np.column_stack([\n",
        "    H1.predict_proba(meta_val[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_val[:, mid:])[:, 1]\n",
        "])\n",
        "meta_for_Hf_test = np.column_stack([\n",
        "    H1.predict_proba(meta_test[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_test[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "# Final blender Hf: StackingClassifier with H1/H2 as estimators\n",
        "final_blender = StackingClassifier(\n",
        "    estimators=[('h1', H1), ('h2', H2)],\n",
        "    final_estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RND),\n",
        "    cv=3,\n",
        "    n_jobs=1\n",
        ")\n",
        "final_blender.fit(meta_for_Hf_val, y_val)\n",
        "\n",
        "# Predicción y evaluación\n",
        "y_pred_test = final_blender.predict(meta_for_Hf_test)\n",
        "acc = accuracy_score(y_test, y_pred_test)\n",
        "f1 = f1_score(y_test, y_pred_test)\n",
        "print(\"SHIA Final - Test Accuracy:\", acc, \"F1:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It7-yjdcDE00",
        "outputId": "29b4e4d4-284a-401d-b3f8-befdbbd5c0a4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIA Final - Test Accuracy: 0.7761194029850746 F1: 0.7272727272727273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:36:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 12) Guardar modelos y registrar en MLflow (local)\n",
        "# ----------------------------\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save base models\n",
        "for name, mdl in models_trained.items():\n",
        "    p = models_dir / f\"{name}.joblib\"\n",
        "    joblib.dump(mdl, p)\n",
        "\n",
        "# Save meta models & final\n",
        "joblib.dump(H1, models_dir / \"H1.joblib\")\n",
        "joblib.dump(H2, models_dir / \"H2.joblib\")\n",
        "joblib.dump(final_blender, models_dir / \"Hf_initial.joblib\")\n",
        "joblib.dump(Hf_new, models_dir / \"Hf_new.joblib\")\n",
        "joblib.dump(Hf_final, models_dir / \"Hf_final.joblib\")\n",
        "print(\"Modelos guardados en\", models_dir.resolve())\n",
        "\n",
        "# MLflow logging (local store)\n",
        "mlflow.set_experiment(\"SHIA_Enterprise_Hybrid\")\n",
        "with mlflow.start_run(run_name=\"shia_hybrid_run\"):\n",
        "    mlflow.log_metric(\"shia_initial_acc\", float(acc))\n",
        "    mlflow.log_metric(\"shia_initial_f1\", float(f1))\n",
        "    mlflow.log_metric(\"shia_incremental_acc\", float(acc_inc))\n",
        "    mlflow.log_metric(\"shia_incremental_f1\", float(f1_inc))\n",
        "    # log artifacts (models)\n",
        "    for file in models_dir.glob(\"*.joblib\"):\n",
        "        mlflow.log_artifact(str(file), artifact_path=\"models\")\n",
        "print(\"Registro MLflow completado. Revisa la carpeta ./mlruns por detalles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs1yTU3EDVUl",
        "outputId": "6965f35c-f1ed-435b-e899-15a0623d8515"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos guardados en /content/models\n",
            "Registro MLflow completado. Revisa la carpeta ./mlruns por detalles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 13) Cleanup recursos\n",
        "# ----------------------------\n",
        "client.close()\n",
        "cluster.close()\n",
        "spark.stop()\n",
        "print(\"Recursos liberados. Fin del pipeline SHIA.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KugRMMVkDZPn",
        "outputId": "53fa7cca-cbbe-49c8-94db-e89074eb1874"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Remove client Client-fe2f31db-adc1-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45554; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-fe2f31db-adc1-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-fe2f31db-adc1-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1760971037.4187758') (0, 1, 2, 3)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41557'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:44279'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43847'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36333'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:40864; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:40872; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:40870; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:46465 name: 0 (stimulus_id='handle-worker-cleanup-1760971037.5773525')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:42435 name: 1 (stimulus_id='handle-worker-cleanup-1760971037.5856802')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:41561 name: 2 (stimulus_id='handle-worker-cleanup-1760971037.5978413')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45550; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:39169 name: 3 (stimulus_id='handle-worker-cleanup-1760971037.6830063')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:43847' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:41557' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:44279' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:36333' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recursos liberados. Fin del pipeline SHIA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f49555"
      },
      "source": [
        "# Task\n",
        "Implement a machine learning pipeline that includes data augmentation, cross-validation, data architecture, and machine learning algorithm generation. The pipeline should train base models, meta-learners, and a final blender, evaluate their performance, simulate the arrival of new data for incremental training, and save the models and log metrics using MLflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc56a463"
      },
      "source": [
        "## Generar meta-features con validación cruzada (para entrenamiento del meta-learner)\n",
        "\n",
        "### Subtask:\n",
        "Utilizar la validación cruzada en `X_train` y `y_train` con los modelos base entrenados para generar las predicciones out-of-fold (OOF). Estas predicciones OOF serán las meta-features para entrenar el meta-learner en la fase inicial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad957720"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires generating out-of-fold predictions using cross-validation on the training data with the trained base models. This involves iterating through the base models, splitting the training data using StratifiedKFold, training a copy of each model on the training folds, predicting on the validation folds, and storing the predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16eb3b9d",
        "outputId": "7f9b8e6b-8cc7-4acb-ef6e-ee84aa724d3c"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RND)\n",
        "\n",
        "# Dictionary to store out-of-fold predictions\n",
        "meta_train_oof = {}\n",
        "\n",
        "# Iterate through base models\n",
        "for name, model in models_trained.items():\n",
        "    print(f\"Generating OOF predictions for {name}...\")\n",
        "    # Array to store OOF predictions for the current model\n",
        "    oof_preds = np.zeros(X_train.shape[0])\n",
        "\n",
        "    # Iterate through cross-validation splits\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "        # Create a copy of the model to train on the current fold\n",
        "        model_copy = model.__class__(**model.get_params())\n",
        "\n",
        "        # Train the model copy on the training fold\n",
        "        model_copy.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
        "\n",
        "        # Make predictions (probabilities for the positive class) on the validation fold\n",
        "        # Check if the model has predict_proba, otherwise use predict\n",
        "        if hasattr(model_copy, 'predict_proba'):\n",
        "            predictions = model_copy.predict_proba(X_train.iloc[val_idx])[:, 1]\n",
        "        else:\n",
        "            predictions = model_copy.predict(X_train.iloc[val_idx])\n",
        "\n",
        "        # Store predictions in the OOF array\n",
        "        oof_preds[val_idx] = predictions\n",
        "\n",
        "    # Store OOF predictions for the current model\n",
        "    meta_train_oof[name] = oof_preds\n",
        "    print(f\"Finished OOF predictions for {name}.\")\n",
        "\n",
        "print(\"\\nMeta-features (OOF predictions) generated:\", meta_train_oof.keys())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating OOF predictions for M1_rf...\n",
            "Finished OOF predictions for M1_rf.\n",
            "Generating OOF predictions for M2_gb...\n",
            "Finished OOF predictions for M2_gb.\n",
            "Generating OOF predictions for M3_svc...\n",
            "Finished OOF predictions for M3_svc.\n",
            "Generating OOF predictions for M4_lr...\n",
            "Finished OOF predictions for M4_lr.\n",
            "\n",
            "Meta-features (OOF predictions) generated: dict_keys(['M1_rf', 'M2_gb', 'M3_svc', 'M4_lr'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6e2eab0"
      },
      "source": [
        "**Reasoning**:\n",
        "The out-of-fold predictions for the training set have been generated. Now, the meta-features for the validation and test sets need to be generated by making predictions on these sets using the *fully trained* base models (trained on the entire training set).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43cc4a50",
        "outputId": "712a3613-1787-407b-b150-b7f2e541a38f"
      },
      "source": [
        "# Generate meta-features for validation and test sets using fully trained base models\n",
        "meta_val_preds = {}\n",
        "meta_test_preds = {}\n",
        "\n",
        "# Iterate through fully trained base models\n",
        "for name, model in models_trained.items():\n",
        "    print(f\"Generating meta-features for validation and test sets using {name}...\")\n",
        "    # Predict probabilities on validation set\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        meta_val_preds[name] = model.predict_proba(X_val)[:, 1]\n",
        "        meta_test_preds[name] = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        meta_val_preds[name] = model.predict(X_val)\n",
        "        meta_test_preds[name] = model.predict(X_test)\n",
        "    print(f\"Finished meta-feature generation for {name}.\")\n",
        "\n",
        "print(\"\\nMeta-features for validation generated:\", meta_val_preds.keys())\n",
        "print(\"Meta-features for test generated:\", meta_test_preds.keys())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating meta-features for validation and test sets using M1_rf...\n",
            "Finished meta-feature generation for M1_rf.\n",
            "Generating meta-features for validation and test sets using M2_gb...\n",
            "Finished meta-feature generation for M2_gb.\n",
            "Generating meta-features for validation and test sets using M3_svc...\n",
            "Finished meta-feature generation for M3_svc.\n",
            "Generating meta-features for validation and test sets using M4_lr...\n",
            "Finished meta-feature generation for M4_lr.\n",
            "\n",
            "Meta-features for validation generated: dict_keys(['M1_rf', 'M2_gb', 'M3_svc', 'M4_lr'])\n",
            "Meta-features for test generated: dict_keys(['M1_rf', 'M2_gb', 'M3_svc', 'M4_lr'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce7861f"
      },
      "source": [
        "**Reasoning**:\n",
        "The OOF predictions for the training set (`meta_train_oof`) and the predictions for the validation and test sets (`meta_val_preds`, `meta_test_preds`) have been generated. These are currently stored as dictionaries where each key is the model name and the value is a NumPy array of predictions. To use these as meta-features for the meta-learners, they need to be stacked into NumPy arrays, where each column represents the predictions from one base model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f7cc25e",
        "outputId": "e1276005-8fc8-46b4-b21d-93484e503e1a"
      },
      "source": [
        "# Stack the meta-features into NumPy arrays\n",
        "meta_train = np.column_stack(list(meta_train_oof.values()))\n",
        "meta_val = np.column_stack(list(meta_val_preds.values()))\n",
        "meta_test = np.column_stack(list(meta_test_preds.values()))\n",
        "\n",
        "print(\"Shape of stacked meta_train:\", meta_train.shape)\n",
        "print(\"Shape of stacked meta_val:\", meta_val.shape)\n",
        "print(\"Shape of stacked meta_test:\", meta_test.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of stacked meta_train: (623, 4)\n",
            "Shape of stacked meta_val: (134, 4)\n",
            "Shape of stacked meta_test: (134, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "789d3972"
      },
      "source": [
        "## Entrenar los meta-learners h1 y h2\n",
        "\n",
        "### Subtask:\n",
        "Entrenar los modelos H1 y H2 utilizando las meta-features generadas a partir de la validación cruzada en el conjunto de entrenamiento (`meta_train_oof`) y el target de entrenamiento (`y_train`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "937d05e5"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the meta-learners H1 and H2 using the generated meta-features from the training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "031072c6",
        "outputId": "ad331d3a-9d4e-4172-c654-babe3b54d1a6"
      },
      "source": [
        "# ----------------------------\n",
        "# 9) Entrenar meta-learners H1 y H2 (Pandas/scikit-learn)\n",
        "#    - Usar meta-features OOF (meta_train) y targets (y_train)\n",
        "# ----------------------------\n",
        "# dividir meta-features en dos para H1/H2\n",
        "# Ensure meta_train is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_train, dict):\n",
        "    # If meta_train is still a dictionary of arrays, stack it\n",
        "    meta_train = np.column_stack(list(meta_train.values()))\n",
        "\n",
        "mid = max(1, meta_train.shape[1] // 2)\n",
        "print(f\"Dividing meta-features at column index: {mid}\")\n",
        "\n",
        "# modelos meta\n",
        "H1 = GradientBoostingClassifier(random_state=RND)\n",
        "H2 = LogisticRegression(max_iter=1000, random_state=RND)\n",
        "\n",
        "# Para entrenar H1/H2 usamos las meta-features generadas con OOF (meta_train) y y_train\n",
        "# Ensure y_train is a numpy array or pandas Series\n",
        "if isinstance(y_train, pd.Series):\n",
        "    y_train_np = y_train.values\n",
        "else:\n",
        "    y_train_np = y_train\n",
        "\n",
        "print(\"Training H1...\")\n",
        "H1.fit(meta_train[:, :mid], y_train_np)\n",
        "print(\"H1 trained.\")\n",
        "\n",
        "print(\"Training H2...\")\n",
        "H2.fit(meta_train[:, mid:], y_train_np)\n",
        "print(\"H2 trained.\")\n",
        "\n",
        "print(\"Meta-learners H1 and H2 trained.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing meta-features at column index: 2\n",
            "Training H1...\n",
            "H1 trained.\n",
            "Training H2...\n",
            "H2 trained.\n",
            "Meta-learners H1 and H2 trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a806eb5e"
      },
      "source": [
        "## Entrenar el blender final hf (inicial)\n",
        "\n",
        "### Subtask:\n",
        "Entrenar el modelo `StackingClassifier` utilizando las predicciones probabilísticas de H1 y H2 sobre el conjunto de validación (`meta_val`) como features de entrada, y el target de validación (`y_val`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31995b13"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate and fit the StackingClassifier using the predictions of H1 and H2 on the validation set as input features and the validation target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c0cfe36",
        "outputId": "e57e64ab-8b83-4849-b3e5-53213e9bb2a5"
      },
      "source": [
        "# ----------------------------\n",
        "# 10) Entrenar meta-learners H1, H2 y blender final Hf (Pandas/scikit-learn)\n",
        "# ----------------------------\n",
        "\n",
        "# features for Hf (outputs probabilísticos de H1/H2 sobre validación)\n",
        "# Ensure meta_val is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_val, dict):\n",
        "    # If meta_val is still a dictionary of arrays, stack it\n",
        "    meta_val = np.column_stack(list(meta_val.values()))\n",
        "\n",
        "# Calculate the mid point based on the number of columns in meta_val\n",
        "mid = max(1, meta_val.shape[1] // 2)\n",
        "\n",
        "# Ensure y_val is a numpy array or pandas Series\n",
        "if isinstance(y_val, pd.Series):\n",
        "    y_val_np = y_val.values\n",
        "else:\n",
        "    y_val_np = y_val\n",
        "\n",
        "# Final blender Hf: StackingClassifier with H1/H2 as estimators\n",
        "final_blender = StackingClassifier(\n",
        "    estimators=[('h1', H1), ('h2', H2)],\n",
        "    final_estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RND),\n",
        "    cv=3,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "print(\"Training final blender (StackingClassifier)...\")\n",
        "# Fit the StackingClassifier to the meta_for_Hf_val (predictions of H1/H2 on validation) and y_val\n",
        "# Note: The previous code block already calculated meta_for_Hf_val, but it was not stored in a variable.\n",
        "# Recalculate meta_for_Hf_val using the trained H1 and H2 models on meta_val.\n",
        "meta_for_Hf_val = np.column_stack([\n",
        "    H1.predict_proba(meta_val[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_val[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "final_blender.fit(meta_for_Hf_val, y_val_np)\n",
        "print(\"Final blender (StackingClassifier) trained.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final blender (StackingClassifier)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:52:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final blender (StackingClassifier) trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6552e7a"
      },
      "source": [
        "## Evaluar el blender final hf (inicial)\n",
        "\n",
        "### Subtask:\n",
        "Evaluar el rendimiento del `StackingClassifier` entrenado utilizando las predicciones probabilísticas de H1 y H2 sobre el conjunto de test (`meta_test`) como features de entrada, y el target de test (`y_test`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f401ea2"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained StackingClassifier on the test set using the predictions of H1 and H2 on meta_test as input features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f4cb9ca",
        "outputId": "79bef62a-861b-495f-e4c1-78594c7b2c8e"
      },
      "source": [
        "# Ensure meta_test is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_test, dict):\n",
        "    # If meta_test is still a dictionary of arrays, stack it\n",
        "    meta_test = np.column_stack(list(meta_test.values()))\n",
        "\n",
        "# Calculate the mid point based on the number of columns in meta_test\n",
        "mid = max(1, meta_test.shape[1] // 2)\n",
        "\n",
        "# Generate the input features for the final blender on the test set\n",
        "meta_for_Hf_test = np.column_stack([\n",
        "    H1.predict_proba(meta_test[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_test[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "# Use the trained final_blender model to make predictions on the test set meta-features\n",
        "y_pred_test = final_blender.predict(meta_for_Hf_test)\n",
        "\n",
        "# Ensure y_test is a numpy array or pandas Series\n",
        "if isinstance(y_test, pd.Series):\n",
        "    y_test_np = y_test.values\n",
        "else:\n",
        "    y_test_np = y_test\n",
        "\n",
        "# Calculate the accuracy score\n",
        "acc = accuracy_score(y_test_np, y_pred_test)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test_np, y_pred_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"SHIA Final - Test Accuracy:\", acc, \"F1:\", f1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIA Final - Test Accuracy: 0.7014925373134329 F1: 0.6078431372549019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5685c3ef"
      },
      "source": [
        "## Simular llegada de nuevos datos y generar meta-features incrementales\n",
        "\n",
        "### Subtask:\n",
        "Simulate the arrival of new data and generate the corresponding meta-features from the predictions of the trained base models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb7f600"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate a synthetic dataset by adding noise to a subset of the training data, create a corresponding target variable by sampling from the training target, and generate meta-features for the new data using the trained base models. Then, stack the meta-features into a NumPy array and print the shapes of the new data, target, and meta-features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23847d9d",
        "outputId": "ad42a2aa-f715-4f5d-f2dd-1b5bde4e6d29"
      },
      "source": [
        "# ----------------------------\n",
        "# 11) Simular llegada de nuevos datos (para entrenamiento incremental)\n",
        "#     - Generar datos sintéticos con ruido\n",
        "#     - Generar meta-features para los nuevos datos\n",
        "# ----------------------------\n",
        "\n",
        "# 1. Generate new synthetic dataset by adding random noise to a subset of the original training data\n",
        "new_data_size = int(0.20 * X_train.shape[0])\n",
        "X_new = X_train.sample(n=new_data_size, random_state=RND, replace=True).reset_index(drop=True)\n",
        "\n",
        "# Calculate standard deviation for noise column-wise\n",
        "std_dev = X_train.std()\n",
        "noise_level = 0.01\n",
        "noise = np.random.normal(0, noise_level * std_dev, size=X_new.shape)\n",
        "X_new = X_new + noise\n",
        "# Ensure numerical columns remain numerical after adding noise\n",
        "X_new = X_new[features]\n",
        "\n",
        "# 2. Create a corresponding target variable for the simulated data\n",
        "y_new = y_train.sample(n=new_data_size, random_state=RND, replace=True).reset_index(drop=True)\n",
        "\n",
        "# 3. Generate meta-features for the new data using the trained base models\n",
        "meta_new_preds = {}\n",
        "for name, model in models_trained.items():\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        meta_new_preds[name] = model.predict_proba(X_new)[:, 1]\n",
        "    else:\n",
        "        meta_new_preds[name] = model.predict(X_new)\n",
        "\n",
        "# 4. Stack the meta-features from meta_new_preds into a NumPy array\n",
        "meta_new = np.column_stack(list(meta_new_preds.values()))\n",
        "\n",
        "# 5. Print the shapes of X_new, y_new, and meta_new\n",
        "print(\"Shape of simulated new data (X_new):\", X_new.shape)\n",
        "print(\"Shape of simulated new target (y_new):\", y_new.shape)\n",
        "print(\"Shape of new meta-features (meta_new):\", meta_new.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of simulated new data (X_new): (124, 6)\n",
            "Shape of simulated new target (y_new): (124,)\n",
            "Shape of new meta-features (meta_new): (124, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf8f2e5"
      },
      "source": [
        "## Entrenar el blender final hf (incremental)\n",
        "\n",
        "### Subtask:\n",
        "Entrenar un nuevo modelo `StackingClassifier` (`Hf_new`) utilizando los datos originales de validación (`meta_for_Hf_val`, `y_val`) y los nuevos datos simulados (`meta_new`, `y_new`) de forma incremental.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2db3880e"
      },
      "source": [
        "**Reasoning**:\n",
        "Concatenate the original validation meta-features and target with the new simulated data and then train a new StackingClassifier incrementally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6727b4a8",
        "outputId": "84841ad4-9760-4b43-f6ef-23aad7be75a9"
      },
      "source": [
        "# 1. Concatenate original validation meta-features and new simulated meta-features\n",
        "# Ensure meta_val is a numpy array\n",
        "if isinstance(meta_val, dict):\n",
        "    meta_val = np.column_stack(list(meta_val.values()))\n",
        "\n",
        "combined_meta = np.vstack([meta_val, meta_new])\n",
        "print(\"Shape of combined meta-features:\", combined_meta.shape)\n",
        "\n",
        "# 2. Concatenate original validation target and new simulated target\n",
        "# Ensure y_val and y_new are numpy arrays\n",
        "if isinstance(y_val, pd.Series):\n",
        "    y_val_np = y_val.values\n",
        "else:\n",
        "    y_val_np = y_val\n",
        "\n",
        "if isinstance(y_new, pd.Series):\n",
        "    y_new_np = y_new.values\n",
        "else:\n",
        "    y_new_np = y_new\n",
        "\n",
        "combined_y = np.concatenate([y_val_np, y_new_np])\n",
        "print(\"Shape of combined target:\", combined_y.shape)\n",
        "\n",
        "\n",
        "# 3. Instantiate a new StackingClassifier model (Hf_new)\n",
        "# Use the same configuration as the initial blender\n",
        "Hf_new = StackingClassifier(\n",
        "    estimators=[('h1', H1), ('h2', H2)],\n",
        "    final_estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RND),\n",
        "    cv=3,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "# 4. Train the new StackingClassifier (Hf_new) using the combined data\n",
        "print(\"Training new StackingClassifier (Hf_new) incrementally...\")\n",
        "# The meta-features for the final blender are the probabilities from H1 and H2.\n",
        "# Need to re-generate these probabilities on the combined_meta data.\n",
        "\n",
        "mid = max(1, combined_meta.shape[1] // 2)\n",
        "\n",
        "meta_for_Hf_combined = np.column_stack([\n",
        "    H1.predict_proba(combined_meta[:, :mid])[:, 1],\n",
        "    H2.predict_proba(combined_meta[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "Hf_new.fit(meta_for_Hf_combined, combined_y)\n",
        "print(\"New StackingClassifier (Hf_new) trained.\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of combined meta-features: (258, 4)\n",
            "Shape of combined target: (258,)\n",
            "Training new StackingClassifier (Hf_new) incrementally...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:53:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New StackingClassifier (Hf_new) trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32d78bf3"
      },
      "source": [
        "## Evaluar el blender final hf (incremental)\n",
        "\n",
        "### Subtask:\n",
        "Evaluar el rendimiento del `StackingClassifier` entrenado incrementalmente utilizando las predicciones probabilísticas de H1 y H2 sobre el conjunto de test (`meta_test`) como features de entrada, y el target de test (`y_test`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05494865"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the incrementally trained StackingClassifier (Hf_new) on the test set using the specified steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0efae3b5",
        "outputId": "fbb77828-3481-4605-dc6a-0a9565a7c488"
      },
      "source": [
        "# Ensure meta_test is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_test, dict):\n",
        "    # If meta_test is still a dictionary of arrays, stack it\n",
        "    meta_test = np.column_stack(list(meta_test.values()))\n",
        "\n",
        "# Calculate the mid point based on the number of columns in meta_test\n",
        "mid = max(1, meta_test.shape[1] // 2)\n",
        "\n",
        "# Generate the input features for the incremental blender on the test set\n",
        "meta_for_Hf_test = np.column_stack([\n",
        "    H1.predict_proba(meta_test[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_test[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "# Use the incrementally trained Hf_new model to make predictions on the test set meta-features\n",
        "y_pred_test_inc = Hf_new.predict(meta_for_Hf_test)\n",
        "\n",
        "# Ensure y_test is a numpy array or pandas Series\n",
        "if isinstance(y_test, pd.Series):\n",
        "    y_test_np = y_test.values\n",
        "else:\n",
        "    y_test_np = y_test\n",
        "\n",
        "# Calculate the accuracy score of the incremental blender\n",
        "acc_inc = accuracy_score(y_test_np, y_pred_test_inc)\n",
        "\n",
        "# Calculate the F1 score of the incremental blender\n",
        "f1_inc = f1_score(y_test_np, y_pred_test_inc)\n",
        "\n",
        "# Print the calculated incremental test accuracy and F1 score\n",
        "print(\"SHIA Incremental - Test Accuracy:\", acc_inc, \"F1:\", f1_inc)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIA Incremental - Test Accuracy: 0.7313432835820896 F1: 0.660377358490566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c48ea3"
      },
      "source": [
        "## Guardar modelos y registrar en mlflow\n",
        "\n",
        "### Subtask:\n",
        "Guardar los modelos base, meta-learners y los diferentes blenders finales entrenados. Registrar las métricas de rendimiento de los blenders inicial e incremental en MLflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68925b4c"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the trained models and log metrics and artifacts using MLflow as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cccb95a4",
        "outputId": "a8493b42-9a22-478d-90dd-9966e531347c"
      },
      "source": [
        "# ----------------------------\n",
        "# 12) Guardar modelos y registrar en MLflow (local)\n",
        "# ----------------------------\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save base models\n",
        "for name, mdl in models_trained.items():\n",
        "    p = models_dir / f\"{name}.joblib\"\n",
        "    joblib.dump(mdl, p)\n",
        "\n",
        "# Save meta models & final\n",
        "joblib.dump(H1, models_dir / \"H1.joblib\")\n",
        "joblib.dump(H2, models_dir / \"H2.joblib\")\n",
        "joblib.dump(final_blender, models_dir / \"Hf_initial.joblib\")\n",
        "joblib.dump(Hf_new, models_dir / \"Hf_new.joblib\")\n",
        "# joblib.dump(Hf_final, models_dir / \"Hf_final.joblib\") # Hf_final was not defined in previous steps\n",
        "print(\"Modelos guardados en\", models_dir.resolve())\n",
        "\n",
        "# MLflow logging (local store)\n",
        "mlflow.set_experiment(\"SHIA_Enterprise_Hybrid\")\n",
        "with mlflow.start_run(run_name=\"shia_hybrid_run\"):\n",
        "    mlflow.log_metric(\"shia_initial_acc\", float(acc))\n",
        "    mlflow.log_metric(\"shia_initial_f1\", float(f1))\n",
        "    mlflow.log_metric(\"shia_incremental_acc\", float(acc_inc))\n",
        "    mlflow.log_metric(\"shia_incremental_f1\", float(f1_inc))\n",
        "    # log artifacts (models)\n",
        "    for file in models_dir.glob(\"*.joblib\"):\n",
        "        mlflow.log_artifact(str(file), artifact_path=\"models\")\n",
        "print(\"Registro MLflow completado. Revisa la carpeta ./mlruns por detalles.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos guardados en /content/models\n",
            "Registro MLflow completado. Revisa la carpeta ./mlruns por detalles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17e92d3"
      },
      "source": [
        "## Liberar recursos\n",
        "\n",
        "### Subtask:\n",
        "Liberar los recursos de Spark y Dask cerrando las sesiones y clusters activos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b391018f"
      },
      "source": [
        "**Reasoning**:\n",
        "Close the Dask client and cluster, and stop the Spark session to release resources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c00811",
        "outputId": "e57b10aa-fe38-4c41-e472-d212ed5a0e26"
      },
      "source": [
        "# ----------------------------\n",
        "# 13) Cleanup recursos\n",
        "# ----------------------------\n",
        "client.close()\n",
        "cluster.close()\n",
        "spark.stop()\n",
        "print(\"Recursos liberados. Fin del pipeline SHIA.\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Remove client Client-edd6db4d-adc2-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:34182; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-edd6db4d-adc2-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-edd6db4d-adc2-11f0-8179-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1760972087.8049347') (0, 1, 2, 3)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:40155'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:38541'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:42467'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:33927'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:34178; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:34152; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:34164; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:38125 name: 0 (stimulus_id='handle-worker-cleanup-1760972087.9076533')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:45183 name: 1 (stimulus_id='handle-worker-cleanup-1760972087.9192166')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:41565 name: 2 (stimulus_id='handle-worker-cleanup-1760972087.9280393')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:34174; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:44009 name: 3 (stimulus_id='handle-worker-cleanup-1760972087.9661777')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:33927' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:42467' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:38541' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:40155' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recursos liberados. Fin del pipeline SHIA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f9ef376"
      },
      "source": [
        "Resumen:\n",
        "Hallazgos Clave del Análisis de Datos\n",
        "Se generaron con éxito las predicciones Out-of-Fold (OOF) para el conjunto de entrenamiento utilizando validación cruzada con los modelos base. Estas predicciones OOF sirven como las meta-características para el entrenamiento inicial del meta-aprendiz.\n",
        "\n",
        "Se generaron las meta-características para los conjuntos de validación y prueba utilizando los modelos base completamente entrenados.\n",
        "\n",
        "Las meta-características se apilaron en arrays de NumPy con formas (623, 4) para el entrenamiento, (134, 4) para la validación y (134, 4) para la prueba.\n",
        "\n",
        "Se entrenaron dos meta-aprendices, H1 (Gradient Boosting Classifier) y H2 (Regresión Logística), utilizando las meta-características OOF del conjunto de entrenamiento. H1 se entrenó en la primera mitad de las meta-características y H2 en la segunda mitad.\n",
        "\n",
        "Se entrenó un StackingClassifier inicial (blender final) utilizando las predicciones probabilísticas de H1 y H2 en el conjunto de validación como características de entrada y el objetivo de validación.\n",
        "\n",
        "El blender final inicial logró una precisión en la prueba de aproximadamente 0.7015 y una puntuación F1 de aproximadamente 0.6078 en el conjunto de prueba.\n",
        "\n",
        "Se generó un nuevo conjunto de datos simulado de tamaño 124 al agregar ruido a un subconjunto de los datos de entrenamiento originales, junto con una variable objetivo correspondiente.\n",
        "\n",
        "Las meta-características para los datos nuevos simulados se generaron utilizando los modelos base entrenados, lo que resultó en un array de NumPy de forma (124, 4).\n",
        "\n",
        "Se entrenó un nuevo StackingClassifier (Hf_new) de forma incremental combinando los datos de validación originales (meta-características y objetivo) con los datos nuevos simulados (meta-características y objetivo).\n",
        "\n",
        "El blender final entrenado incrementalmente logró una precisión en la prueba de aproximadamente 0.7313 y una puntuación F1 de aproximadamente 0.6604 en el conjunto de prueba.\n",
        "\n",
        "Todos los modelos entrenados (modelos base, H1, H2, blender inicial y blender incremental) se guardaron con éxito en un directorio local de \"models\".\n",
        "\n",
        "Las métricas de rendimiento (precisión y puntuación F1) para el blender inicial y el incremental se registraron con éxito en MLflow, junto con los archivos de modelo guardados como artefactos.\n",
        "\n",
        "Los recursos de Spark y Dask se liberaron con éxito cerrando las sesiones y clústeres activos.\n",
        "\n",
        "Conclusiones\n",
        "El proceso de entrenamiento incremental utilizando datos nuevos simulados resultó en una mejora tanto en la precisión como en la puntuación F1 en el conjunto de prueba en comparación con el blender inicial. Esto sugiere que la incorporación de datos nuevos puede mejorar el rendimiento del modelo.\n",
        "\n",
        "El registro de MLflow proporciona un registro valioso del proceso de entrenamiento del modelo y las métricas de rendimiento, que puede utilizarse para el seguimiento de experimentos, la comparación de diferentes versiones de modelos y la reproducción de resultados."
      ]
    }
  ]
}