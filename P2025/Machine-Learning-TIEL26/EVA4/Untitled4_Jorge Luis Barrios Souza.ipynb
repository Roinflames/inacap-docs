{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación 4 – Machine Learning (MLOps)\n",
        "## Aplicación de MLOps al modelo de la Evaluación 3 (MNIST)\n",
        "\n",
        "**Estudiante:** Jorge Barrios\n",
        "**Fecha:** 05-12-2025  \n",
        "\n",
        "En esta evaluación, aplico conceptos de **MLOps** al modelo de clasificación de dígitos MNIST utilizado en la Evaluación 3.\n",
        "\n",
        "El objetivo es:\n",
        "\n",
        "- Determinar qué elementos del proceso de ML se deben **monitorear** y **versionar**.\n",
        "- Aplicar **versionamiento** y **trazabilidad** sobre datos, código, modelos y métricas.\n",
        "- Identificar qué tareas del flujo original pueden ser **automatizadas**.\n",
        "- Implementar una **automatización básica** mediante funciones y scripts que permitan repetir el entrenamiento de forma reproducible.\n",
        "\n",
        "El dataset utilizado es **MNIST**, cargado desde `keras.datasets.mnist`, tal como en la Evaluación 3:\n",
        "- Imágenes de dígitos escritos a mano (28x28 píxeles, en escala de grises).\n",
        "- 10 clases (dígitos de 0 a 9).\n"
      ],
      "metadata": {
        "id": "EW-scDtbxn-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10vSIq1mxm26",
        "outputId": "aa47f63a-80fc-40d1-dc03-0c308289aaa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.9/753.9 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mlflow scikit-learn tensorflow pandas numpy matplotlib -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def cargar_dataset():\n",
        "    \"\"\"\n",
        "    Carga el dataset MNIST (el mismo que en la Evaluación 3) y lo transforma\n",
        "    en un DataFrame:\n",
        "      - X: imágenes 28x28 aplanadas a 784 columnas.\n",
        "      - y: dígitos (0-9) en la columna 'target'.\n",
        "      - Normalización de pixeles a [0, 1].\n",
        "    \"\"\"\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Unir train + test para tener un único dataset\n",
        "    X = np.concatenate([x_train, x_test], axis=0)\n",
        "    y = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "    # Aplanar imágenes 28x28 -> 784 y normalizar\n",
        "    X = X.reshape((X.shape[0], -1)).astype(\"float32\") / 255.0\n",
        "\n",
        "    # DataFrame: 784 columnas de features + 1 columna 'target'\n",
        "    df = pd.DataFrame(X)\n",
        "    df[\"target\"] = y\n",
        "\n",
        "    return df\n",
        "\n",
        "def resumir_dataset(df: pd.DataFrame):\n",
        "    print(\"Shape:\", df.shape)\n",
        "    print(\"\\nTipos de datos (primeras columnas):\")\n",
        "    print(df.dtypes.head())\n",
        "    print(\"\\nValores nulos por columna (primeras columnas):\")\n",
        "    print(df.isna().sum().head())\n",
        "\n",
        "def hash_dataframe(df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Calcula un hash MD5 del contenido del DataFrame para simular\n",
        "    versionamiento de datos (detecta cambios en el dataset).\n",
        "    \"\"\"\n",
        "    df_bytes = pd.util.hash_pandas_object(df, index=True).values\n",
        "    return hashlib.md5(df_bytes).hexdigest()\n",
        "\n",
        "def validar_dataset(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Validación automática básica del dataset.\n",
        "    - Verifica que no esté vacío.\n",
        "    - Verifica que no haya valores nulos.\n",
        "    - Verifica que las features estén en el rango [0, 1].\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Dataset vacío\")\n",
        "\n",
        "    if df.isna().sum().sum() > 0:\n",
        "        print(\"Advertencia: hay valores nulos, se requiere imputación.\")\n",
        "    else:\n",
        "        print(\"OK: no hay valores nulos.\")\n",
        "\n",
        "    # Chequeo simple de rangos (features deben estar entre 0 y 1)\n",
        "    num_cols = df.drop(columns=[\"target\"])\n",
        "    min_val = num_cols.min().min()\n",
        "    max_val = num_cols.max().max()\n",
        "\n",
        "    if min_val < 0 or max_val > 1:\n",
        "        print(f\"Advertencia: hay valores fuera de [0, 1]. min={min_val}, max={max_val}\")\n",
        "    else:\n",
        "        print(\"OK: todas las features están en [0, 1].\")\n",
        "\n",
        "    print(\"Validación básica completada.\")\n"
      ],
      "metadata": {
        "id": "TkL9Y_Sdx0-1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = cargar_dataset()\n",
        "resumir_dataset(df)\n",
        "dataset_hash = hash_dataframe(df)\n",
        "print(\"\\nHash del dataset:\", dataset_hash)\n",
        "validar_dataset(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVH_wwmvx_Sf",
        "outputId": "be82bd64-fa32-4b6f-9bbb-7f8e5e038a09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape: (70000, 785)\n",
            "\n",
            "Tipos de datos (primeras columnas):\n",
            "0    float32\n",
            "1    float32\n",
            "2    float32\n",
            "3    float32\n",
            "4    float32\n",
            "dtype: object\n",
            "\n",
            "Valores nulos por columna (primeras columnas):\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "dtype: int64\n",
            "\n",
            "Hash del dataset: 7cd394616d2ece8836deb9648901b599\n",
            "OK: no hay valores nulos.\n",
            "OK: todas las features están en [0, 1].\n",
            "Validación básica completada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Elementos a monitorear y versionar\n",
        "\n",
        "En el contexto del modelo MNIST de la Evaluación 3, decido monitorear y versionar los siguientes elementos:\n",
        "\n",
        "### 1.1 Datos (Dataset MNIST)\n",
        "\n",
        "- **Origen de datos**: `keras.datasets.mnist`.\n",
        "- **Tamaño**: número de filas y columnas (`df.shape`).\n",
        "- **Esquema**: nombres y tipos de las columnas.\n",
        "- **Transformaciones**: aplanamiento de las imágenes (28x28 → 784) y normalización a [0, 1].\n",
        "- **Hash del dataset**: se calcula un `hash` (MD5) del DataFrame para detectar cambios en el contenido.\n",
        "\n",
        "### 1.2 Modelo\n",
        "\n",
        "- Tipo de modelo: en este caso uso un **RandomForestClassifier** sobre las features aplanadas, como ejemplo de modelo clásico aplicado a MNIST.\n",
        "- **Hiperparámetros** principales:\n",
        "  - `n_estimators`\n",
        "  - `max_depth`\n",
        "  - `test_size` usado en el split\n",
        "- Versión lógica del modelo: a través de **runs** en MLflow (por ejemplo `run_id`, nombre del experimento).\n",
        "\n",
        "### 1.3 Métricas\n",
        "\n",
        "- Métricas principales del modelo:\n",
        "  - `accuracy`\n",
        "  - `f1_score` (macro o weighted, según configuración)\n",
        "- Otros outputs:\n",
        "  - Reporte de clasificación (precision, recall) que se puede guardar como artefacto textual.\n",
        "\n",
        "### 1.4 Ambiente y dependencias\n",
        "\n",
        "- Versión de Python.\n",
        "- Versiones de librerías: `scikit-learn`, `pandas`, `numpy`, `tensorflow`, `mlflow`, etc.\n",
        "- Archivo `requirements.txt` con las dependencias mínimas.\n",
        "\n",
        "### 1.5 Preprocesamiento\n",
        "\n",
        "- Pasos de preparación de los datos:\n",
        "  - Reescalado de features (si corresponde).\n",
        "  - División train/test.\n",
        "- En este ejemplo, el RandomForest no requiere escalado explícito, pero dejo preparado el pipeline con posibilidad de incluir `StandardScaler`.\n",
        "\n",
        "### 1.6 Logs y ejecución\n",
        "\n",
        "- Fecha y hora de entrenamiento.\n",
        "- Identificador del experimento (run) en MLflow.\n",
        "- Mensajes de logging impresos durante el entrenamiento (accuracy, F1, etc.).\n",
        "\n",
        "Para la **trazabilidad completa**:\n",
        "- Uso **Git** para versionar el código del notebook y los scripts (`main_pipeline.py`, `run_pipeline.py`).\n",
        "- Uso **MLflow** para registrar parámetros, métricas, hash del dataset y artefactos del modelo.\n"
      ],
      "metadata": {
        "id": "ST1G0kMwyBnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "def dividir_dataset(df: pd.DataFrame, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Separa el dataset en train y test.\n",
        "    \"\"\"\n",
        "    X = df.drop(columns=[\"target\"])\n",
        "    y = df[\"target\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def construir_pipeline(n_estimators=100, max_depth=None):\n",
        "    \"\"\"\n",
        "    Construye un pipeline simple:\n",
        "      - (Opcional) escalado.\n",
        "      - RandomForestClassifier.\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline(steps=[\n",
        "        (\"scaler\", StandardScaler(with_mean=False)),  # with_mean=False por sparse posible\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ])\n",
        "    return pipeline\n"
      ],
      "metadata": {
        "id": "cELPjPR5yD5O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuración de MLflow (tracking local en la carpeta mlruns)\n",
        "tracking_uri = os.path.join(os.getcwd(), \"mlruns\")\n",
        "mlflow.set_tracking_uri(f\"file:{tracking_uri}\")\n",
        "mlflow.set_experiment(\"eva4_mnist_mlops\")\n",
        "\n",
        "def entrenar_y_registrar_modelo(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    test_size=0.2,\n",
        "):\n",
        "    \"\"\"\n",
        "    Entrena el modelo sobre MNIST, valida el dataset,\n",
        "    registra todo en MLflow y devuelve el modelo + métricas.\n",
        "    \"\"\"\n",
        "    # Cargar y validar datos\n",
        "    df = cargar_dataset()\n",
        "    validar_dataset(df)\n",
        "\n",
        "    # División train/test\n",
        "    X_train, X_test, y_train, y_test = dividir_dataset(df, test_size=test_size)\n",
        "\n",
        "    # Comenzar un run de MLflow\n",
        "    with mlflow.start_run(run_name=f\"mnist_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
        "        model = construir_pipeline(n_estimators=n_estimators, max_depth=max_depth)\n",
        "\n",
        "        # Entrenamiento\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predicciones\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Métricas\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "        # ---- Log de parámetros ----\n",
        "        mlflow.log_param(\"n_estimators\", n_estimators)\n",
        "        mlflow.log_param(\"max_depth\", max_depth)\n",
        "        mlflow.log_param(\"test_size\", test_size)\n",
        "        mlflow.log_param(\"dataset_hash\", hash_dataframe(df))\n",
        "\n",
        "        # ---- Log de métricas ----\n",
        "        mlflow.log_metric(\"accuracy\", acc)\n",
        "        mlflow.log_metric(\"f1_macro\", f1)\n",
        "\n",
        "        # ---- Log de artefactos ----\n",
        "        # Guardar el modelo\n",
        "        mlflow.sklearn.log_model(model, artifact_path=\"modelo\")\n",
        "\n",
        "        # Guardar un reporte de clasificación\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        report_path = \"reporte_clasificacion.txt\"\n",
        "        with open(report_path, \"w\") as f:\n",
        "            f.write(\"Reporte de clasificación (MNIST):\\n\")\n",
        "            f.write(report)\n",
        "            f.write(\"\\n\")\n",
        "            f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
        "            f.write(f\"F1 macro: {f1:.4f}\\n\")\n",
        "        mlflow.log_artifact(report_path)\n",
        "\n",
        "        print(\"Run ID:\", run.info.run_id)\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(f\"F1-score (macro): {f1:.4f}\")\n",
        "\n",
        "    return model, acc, f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP9TLglNyOq0",
        "outputId": "68a9d4f8-e50a-4ba5-958a-8a66ae51c751"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "2025/12/05 18:32:00 INFO mlflow.tracking.fluent: Experiment with name 'eva4_mnist_mlops' does not exist. Creating a new experiment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo, acc, f1 = entrenar_y_registrar_modelo(\n",
        "    n_estimators=120,\n",
        "    max_depth=20,\n",
        "    test_size=0.2\n",
        ")\n",
        "\n",
        "print(\"\\nEntrenamiento completado.\")\n",
        "print(f\"Accuracy final: {acc:.4f}\")\n",
        "print(f\"F1-score macro final: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQJ4WlcByUBU",
        "outputId": "c6b53c89-e061-4d6e-df1e-59eae14ffc9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: no hay valores nulos.\n",
            "OK: todas las features están en [0, 1].\n",
            "Validación básica completada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/12/05 18:33:09 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run ID: 434f4667783141f89c95d977365a0516\n",
            "Accuracy: 0.9669\n",
            "F1-score (macro): 0.9666\n",
            "\n",
            "Entrenamiento completado.\n",
            "Accuracy final: 0.9669\n",
            "F1-score macro final: 0.9666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tareas a automatizar (poda de actividades manuales)\n",
        "\n",
        "Antes de aplicar MLOps, el flujo típico del modelo de MNIST incluía varias tareas manuales:\n",
        "\n",
        "1. **Carga del dataset**:\n",
        "   - Ejecutar manualmente el código de `keras.datasets.mnist.load_data()`.\n",
        "   - Aplicar manualmente la transformación de las imágenes.\n",
        "\n",
        "2. **Preprocesamiento y validación**:\n",
        "   - Revisar manualmente si el dataset tiene nulos o valores fuera de rango.\n",
        "   - Dividir manualmente en train/test.\n",
        "\n",
        "3. **Entrenamiento del modelo**:\n",
        "   - Escribir el código del modelo y ejecutar las celdas a mano.\n",
        "   - Ajustar hiperparámetros probando distintas celdas.\n",
        "\n",
        "4. **Evaluación y métricas**:\n",
        "   - Calcular accuracy/F1 manualmente.\n",
        "   - Mirar los resultados sin registrarlos de forma estructurada.\n",
        "\n",
        "5. **Guardado del modelo y resultados**:\n",
        "   - Guardar el modelo con `joblib`/`pickle` a mano.\n",
        "   - Anotar resultados en otro lado (documento, Excel, etc.).\n",
        "\n",
        "### 2.1. Decisiones de automatización\n",
        "\n",
        "Para reducir errores y hacer el proceso reproducible, automatizo:\n",
        "\n",
        "- **Validación del dataset**:\n",
        "  - La función `validar_dataset(df)` se ejecuta siempre antes de entrenar.\n",
        "\n",
        "- **Entrenamiento completo en una sola función**:\n",
        "  - `entrenar_y_registrar_modelo(...)` encapsula:\n",
        "    - Carga de datos.\n",
        "    - Validación.\n",
        "    - División train/test.\n",
        "    - Entrenamiento.\n",
        "    - Cálculo de métricas.\n",
        "    - Registro en MLflow.\n",
        "\n",
        "- **Versionamiento y trazabilidad**:\n",
        "  - Cada corrida queda registrada en **MLflow** con parámetros, métricas y `dataset_hash`.\n",
        "  - El modelo entrenado se guarda como **artefacto**.\n",
        "\n",
        "- **Script ejecutable único**:\n",
        "  - Defino un script `run_pipeline.py` que permite entrenar el modelo ejecutando un solo comando:\n",
        "    - `python run_pipeline.py`\n",
        "\n",
        "Con estas automatizaciones, el entrenamiento del modelo MNIST deja de depender de pasos manuales dispersos y pasa a ser un pipeline reproducible y auditable.\n"
      ],
      "metadata": {
        "id": "hKcyXHt0z1VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main_pipeline.py\n",
        "import os\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "\n",
        "def cargar_dataset():\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    X = np.concatenate([x_train, x_test], axis=0)\n",
        "    y = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "    X = X.reshape((X.shape[0], -1)).astype(\"float32\") / 255.0\n",
        "\n",
        "    df = pd.DataFrame(X)\n",
        "    df[\"target\"] = y\n",
        "    return df\n",
        "\n",
        "\n",
        "def hash_dataframe(df: pd.DataFrame) -> str:\n",
        "    df_bytes = pd.util.hash_pandas_object(df, index=True).values\n",
        "    return hashlib.md5(df_bytes).hexdigest()\n",
        "\n",
        "\n",
        "def validar_dataset(df: pd.DataFrame):\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Dataset vacío\")\n",
        "\n",
        "    if df.isna().sum().sum() > 0:\n",
        "        print(\"Advertencia: hay valores nulos, se requiere imputación.\")\n",
        "    else:\n",
        "        print(\"OK: no hay valores nulos.\")\n",
        "\n",
        "    num_cols = df.drop(columns=[\"target\"])\n",
        "    min_val = num_cols.min().min()\n",
        "    max_val = num_cols.max().max()\n",
        "\n",
        "    if min_val < 0 or max_val > 1:\n",
        "        print(f\"Advertencia: hay valores fuera de [0, 1]. min={min_val}, max={max_val}\")\n",
        "    else:\n",
        "        print(\"OK: todas las features están en [0, 1].\")\n",
        "\n",
        "    print(\"Validación básica completada.\")\n",
        "\n",
        "\n",
        "def dividir_dataset(df: pd.DataFrame, test_size=0.2):\n",
        "    X = df.drop(columns=[\"target\"])\n",
        "    y = df[\"target\"]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def construir_pipeline(n_estimators=100, max_depth=None):\n",
        "    pipeline = Pipeline(steps=[\n",
        "        (\"scaler\", StandardScaler(with_mean=False)),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# Configuración de MLflow\n",
        "tracking_uri = os.path.join(os.getcwd(), \"mlruns\")\n",
        "mlflow.set_tracking_uri(f\"file:{tracking_uri}\")\n",
        "mlflow.set_experiment(\"eva4_mnist_mlops_script\")\n",
        "\n",
        "\n",
        "def entrenar_y_registrar_modelo(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    test_size=0.2,\n",
        "):\n",
        "    df = cargar_dataset()\n",
        "    validar_dataset(df)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = dividir_dataset(df, test_size=test_size)\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"mnist_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
        "        model = construir_pipeline(n_estimators=n_estimators, max_depth=max_depth)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "        mlflow.log_param(\"n_estimators\", n_estimators)\n",
        "        mlflow.log_param(\"max_depth\", max_depth)\n",
        "        mlflow.log_param(\"test_size\", test_size)\n",
        "        mlflow.log_param(\"dataset_hash\", hash_dataframe(df))\n",
        "\n",
        "        mlflow.log_metric(\"accuracy\", acc)\n",
        "        mlflow.log_metric(\"f1_macro\", f1)\n",
        "\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        report_path = \"reporte_clasificacion.txt\"\n",
        "        with open(report_path, \"w\") as f:\n",
        "            f.write(\"Reporte de clasificación (MNIST):\\n\")\n",
        "            f.write(report)\n",
        "            f.write(\"\\n\")\n",
        "            f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
        "            f.write(f\"F1 macro: {f1:.4f}\\n\")\n",
        "        mlflow.log_artifact(report_path)\n",
        "\n",
        "        mlflow.sklearn.log_model(model, artifact_path=\"modelo\")\n",
        "\n",
        "        print(\"Run ID:\", run.info.run_id)\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(f\"F1-score (macro): {f1:.4f}\")\n",
        "\n",
        "    return model, acc, f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCpx2rUz4la",
        "outputId": "ede4178a-60d0-4b84-ed95-2e8a42b82961"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_pipeline.py\n",
        "from main_pipeline import entrenar_y_registrar_modelo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Hiperparámetros por defecto (podrían venir de variables de entorno)\n",
        "    entrenar_y_registrar_modelo(\n",
        "        n_estimators=150,\n",
        "        max_depth=25,\n",
        "        test_size=0.2,\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wJJziTt0AXd",
        "outputId": "a4643781-11b7-49a9-ca68-eb57270e5b82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_pipeline.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJdLoggN0CTm",
        "outputId": "d062187f-878c-422e-a3b9-6568ab0b649c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-05 18:39:50.373693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764959990.451266    3301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764959990.475299    3301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764959990.538108    3301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764959990.538212    3301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764959990.538227    3301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764959990.538237    3301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "2025/12/05 18:40:01 INFO mlflow.tracking.fluent: Experiment with name 'eva4_mnist_mlops_script' does not exist. Creating a new experiment.\n",
            "OK: no hay valores nulos.\n",
            "OK: todas las features están en [0, 1].\n",
            "Validación básica completada.\n",
            "2025/12/05 18:41:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "Run ID: b79c64656bc24ffcab85d1c2e149df2c\n",
            "Accuracy: 0.9676\n",
            "F1-score (macro): 0.9673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "mlflow\n",
        "scikit-learn\n",
        "tensorflow\n",
        "pandas\n",
        "numpy\n",
        "matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cu3btHo0ahp",
        "outputId": "584f0168-90e0-47b6-bc75-c2d7a88361ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\".github/workflows\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Mrwsqdyv0dSI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .github/workflows/train.yml\n",
        "name: train-mnist-model\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [ main ]\n",
        "  workflow_dispatch:\n",
        "\n",
        "jobs:\n",
        "  train:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "      - name: Checkout repository\n",
        "        uses: actions/checkout@v4\n",
        "\n",
        "      - name: Set up Python\n",
        "        uses: actions/setup-python@v5\n",
        "        with:\n",
        "          python-version: '3.10'\n",
        "\n",
        "      - name: Install dependencies\n",
        "        run: |\n",
        "          pip install -r requirements.txt\n",
        "\n",
        "      - name: Run training pipeline\n",
        "        run: |\n",
        "          python run_pipeline.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqvtv_WB0fTn",
        "outputId": "a0419a03-4ba4-438a-ab58-2a2ce97339b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .github/workflows/train.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git status\n",
        "!git add main_pipeline.py run_pipeline.py requirements.txt .github/train.yml 2>/dev/null || echo \"Archivos listos para versionar.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLUG5EUH0kCU",
        "outputId": "1ed9291d-826e-4eae-a444-ff1aa0eb0739"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "On branch master\n",
            "\n",
            "No commits yet\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31m.github/\u001b[m\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\t\u001b[31mmain_pipeline.py\u001b[m\n",
            "\t\u001b[31mmlruns/\u001b[m\n",
            "\t\u001b[31mreporte_clasificacion.txt\u001b[m\n",
            "\t\u001b[31mrequirements.txt\u001b[m\n",
            "\t\u001b[31mrun_pipeline.py\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "Archivos listos para versionar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commit simple (puede fallar si no configuras user.name / user.email, es normal en Colab)\n",
        "!git commit -m \"EVA4 - Pipeline MNIST con MLOps\" || echo(\"Commit no ejecutado (falta configurar git), pero el flujo de versionamiento está descrito.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M85qqta0oBZ",
        "outputId": "10dfa938-67e0-48c7-9588-7d497246aaa8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `\"Commit no ejecutado (falta configurar git), pero el flujo de versionamiento está descrito.\"'\n",
            "/bin/bash: -c: line 1: `git commit -m \"EVA4 - Pipeline MNIST con MLOps\" || echo(\"Commit no ejecutado (falta configurar git), pero el flujo de versionamiento está descrito.\")'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Conclusiones\n",
        "\n",
        "En esta Evaluación 4 apliqué prácticas de **MLOps** al modelo de clasificación de dígitos MNIST utilizado en la Evaluación 3:\n",
        "\n",
        "- **Elementos monitoreados y versionados**:\n",
        "  - Datos (MNIST) con validación básica y cálculo de `hash` para detectar cambios.\n",
        "  - Modelo (RandomForest sobre imágenes aplanadas) con hiperparámetros registrados.\n",
        "  - Métricas (accuracy y F1-score macro) para comparar experimentos.\n",
        "  - Ambiente y dependencias (requirements.txt).\n",
        "\n",
        "- **Versionamiento y trazabilidad**:\n",
        "  - Uso de **MLflow** como sistema de tracking local (experimentos, parámetros, métricas y artefactos).\n",
        "  - Uso de **Git** para versionar código y archivos auxiliares (`main_pipeline.py`, `run_pipeline.py`, `requirements.txt`, workflow de GitHub Actions).\n",
        "\n",
        "- **Poda y automatización de tareas**:\n",
        "  - Encapsulé el proceso completo de entrenamiento en `entrenar_y_registrar_modelo`, reduciendo pasos manuales dispersos.\n",
        "  - Definí un script ejecutable `run_pipeline.py` para poder lanzar el entrenamiento con un único comando.\n",
        "  - Preparé un workflow de **CI/CD (GitHub Actions)** que instala dependencias y ejecuta el pipeline automáticamente ante un `push` o disparo manual.\n",
        "\n",
        "Con esto, el proceso de entrenamiento y evaluación del modelo MNIST deja de ser un conjunto de celdas aisladas y pasa a ser un **pipeline reproducible, trazable y automatizable**, alineado con los objetivos de MLOps propuestos en la evaluación.\n"
      ],
      "metadata": {
        "id": "WLGRFW4_0rcK"
      }
    }
  ]
}