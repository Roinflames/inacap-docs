{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IntroducciÃ³n\n",
        "\n",
        "Aqui econ al eva_3 nuestro trabajo tiene el objetivo implementar un modelo Autoencoder utilizando el conjunto de datos Fashion-MNIST, aplicando los conocimientos vistos en clase sobre aprendizaje no supervisado y redes neuronales.\n",
        "En el notebook se describen paso a paso los procesos solicitados en la evaluaciÃ³n: carga del dataset, definiciÃ³n de dimensiones, construcciÃ³n del codificador y decodificador, creaciÃ³n de modelos, entrenamiento y anÃ¡lisis de resultados.\n",
        "\n",
        "**El propÃ³sito de este modelo es aprender una representaciÃ³n comprimida de las imÃ¡genes de prendas de vestir para luego reconstruirlas de la forma mÃ¡s fiel posible. De esta manera, se puede entender cÃ³mo una red neuronal aprende a representar informaciÃ³n de manera eficiente sin usar etiquetas**.\n",
        "\n",
        "https://github.com/zalandoresearch/fashion-mnist"
      ],
      "metadata": {
        "id": "SIm3XQt5zcKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cargar el conjunto de datos\n",
        "\n",
        "En esta celda cargamos los datos del conjunto Fashion-MNIST, un dataset incluido en Keras que contiene 70 000 imÃ¡genes en escala de grises de 28x28 pÃ­xeles, divididas en 60 000 para entrenamiento y 10 000 para prueba.\n",
        "\n",
        "Cada imagen representa un tipo de prenda (como zapatilla, polera o abrigo), pero en este proyecto solo se usan las imÃ¡genes para entrenar el modelo de reconstrucciÃ³n, no las etiquetas.\n",
        "\n",
        "El objetivo es obtener una base de datos lista para alimentar el autoencoder."
      ],
      "metadata": {
        "id": "mtZDLbmezozj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Cargar Fashion-MNIST\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "print(\"Dimensiones de x_train:\", x_train.shape)\n",
        "print(\"Dimensiones de x_test: \", x_test.shape)\n",
        "\n",
        "# Normalizar los valores de pÃ­xeles a [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test  = x_test.astype('float32')  / 255.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ8d626apUyk",
        "outputId": "5cc6c891-3a82-49a4-a00f-7f371a3e63c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Dimensiones de x_train: (60000, 28, 28)\n",
            "Dimensiones de x_test:  (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al ejecutar esto, deberÃ­amos ver que x_train tiene dimensiÃ³n (60000, 28, 28) y x_test (10000, 28, 28)\n",
        "tensorflow.org\n",
        ", confirmando 60k imÃ¡genes de entrenamiento y 10k de prueba, cada una de 28x28 pÃ­xeles."
      ],
      "metadata": {
        "id": "H9zcZHTRpqFP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHBAxO6MppmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensiones de la entrada y la representaciÃ³n latente\n",
        "\n",
        "Antes de construir el autoencoder, debemos entender las **dimensiones** de los datos, es decir, el **tamaÃ±o o cantidad de valores** que describen a cada imagen.\n",
        "\n",
        "Cada imagen del conjunto **Fashion-MNIST** tiene un tamaÃ±o de **28 x 28 pÃ­xeles**.  \n",
        "Si multiplicamos esas dimensiones:  \n",
        "> 28 Ã— 28 = **784**\n",
        "\n",
        "Eso significa que **cada imagen se representa como un vector de 784 nÃºmeros**, donde cada nÃºmero indica la intensidad (de 0 a 1) de un pÃ­xel.\n",
        "\n",
        "En otras palabras:\n",
        "- **784** â†’ cantidad de datos de entrada (cada pÃ­xel es una caracterÃ­stica)\n",
        "- Cada imagen tiene 784 â€œdimensionesâ€ porque el modelo ve 784 valores distintos.\n",
        "\n",
        "Ahora bien, el **codificador** del autoencoder **no necesita conservar los 784 valores originales**.  \n",
        "Su funciÃ³n es **comprimir la informaciÃ³n mÃ¡s importante** de cada imagen en un nÃºmero mucho menor de valores: eso se llama **representaciÃ³n latente (comprimida)**.\n",
        "\n",
        "En este proyecto, elegimos una **representaciÃ³n de 64 dimensiones**.  \n",
        "Esto significa que el modelo convertirÃ¡ cada imagen (de 784 valores) en un vector **de solo 64 nÃºmeros**.\n",
        "\n",
        " resultado de lo mas releavnte o imortante:\n",
        "\n",
        "> ** Entrada:** 784 valores â†’ **Salida codificada (latente):** 64 valores\n",
        "\n",
        "Nuestro modelo aprende a conservar solo los patrones mÃ¡s relevantes (por ejemplo, el contorno de una camisa o la forma de un zapato) dentro de esos 64 nÃºmeros.\n",
        "\n",
        "Luego, el **decodificador** usa esos 64 valores para **reconstruir los 784 originales**, intentando recuperar la imagen lo mejor posible.\n",
        "\n",
        "resumen:\n",
        "- â€œ784â€ es el tamaÃ±o original (todas las intensidades de los pÃ­xeles).\n",
        "- â€œ64â€ es el tamaÃ±o del vector comprimido aprendido por el modelo.\n",
        "- Esta reducciÃ³n de **784 â†’ 64** obliga al autoencoder a **aprender las caracterÃ­sticas esenciales** de las imÃ¡genes en lugar de memorizar los pÃ­xeles.\n"
      ],
      "metadata": {
        "id": "JiK86g9cqLJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir dimensiones\n",
        "input_shape = x_train.shape[1:]   # (28, 28)\n",
        "input_dim   = input_shape[0] * input_shape[1]  # 784\n",
        "latent_dim  = 64\n",
        "\n",
        "print(\"DimensiÃ³n de entrada (flattened):\", input_dim)\n",
        "print(\"DimensiÃ³n de la representaciÃ³n latente:\", latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxnkewesqZCN",
        "outputId": "7dff7675-0010-43fe-cf4a-dd4347d1ca6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimensiÃ³n de entrada (flattened): 784\n",
            "DimensiÃ³n de la representaciÃ³n latente: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos confirmarÃ¡: 784 de entrada y 64 latente."
      ],
      "metadata": {
        "id": "6HOLzQ6aqm1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir el codificador y decodificador\n",
        "\n",
        "En este paso creamos las dos partes del modelo: **el codificador (encoder)** y **el decodificador (decoder)**.  \n",
        "Cada uno cumple una funciÃ³n diferente:\n",
        "\n",
        "-  **Codificador (encoder):**  \n",
        "  Toma la imagen original (28x28 pÃ­xeles), la aplana a un vector de 784 valores y la pasa por una capa densa de **64 neuronas** con activaciÃ³n **ReLU**(ayuda al aprendizaje rÃ¡pido eliminando valores negativos.  ).  \n",
        "  El resultado es el **vector latente de 64 dimensiones**, la versiÃ³n comprimida de la imagen.  \n",
        "  \n",
        "\n",
        "- **Decodificador (decoder):**  \n",
        "  Recibe ese vector comprimido (64 valores) y lo expande nuevamente a **784 neuronas** con activaciÃ³n **sigmoid**, para reconstruir la imagen de 28x28.  \n",
        "  \n",
        "\n",
        "Usamos la **API funcional de Keras** porque nos permite conectar fÃ¡cilmente las dos partes y tambiÃ©n crear cada una por separado (por ejemplo, tener solo el encoder para inspeccionar el vector latente).\n",
        "\n",
        "Normalizamos los **pÃ­xeles entre 0 y 1** con activaciÃ³n **sigmoid**,para que el modelo aprenda mejor, ya que su salida tambiÃ©n estÃ¡ en ese rango.  \n",
        "AsÃ­, el error cuadrÃ¡tico medio puede medir con precisiÃ³n la diferencia entre la imagen reconstruida y la original.\n",
        "\n",
        "\n",
        "Implementemos esto:"
      ],
      "metadata": {
        "id": "kDEx_gZ3q_cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Flatten, Dense, Reshape\n",
        "\n",
        "# Definir codificador\n",
        "encoder_input = Input(shape=input_shape, name=\"encoder_input\")\n",
        "x = Flatten()(encoder_input)\n",
        "encoder_output = Dense(latent_dim, activation='relu', name=\"latent_vector\")(x)\n",
        "encoder_model = Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "print(encoder_model.summary())\n",
        "\n",
        "# Definir decodificador\n",
        "decoder_input = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
        "x = Dense(input_dim, activation='sigmoid')(decoder_input)  # output layer (784,)\n",
        "decoder_output = Reshape(input_shape)(x)  # reshape to (28,28)\n",
        "decoder_model = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "print(decoder_model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "2xg6gTDvrBWn",
        "outputId": "f384311c-3e34-48d8-a726-05f955b84bf0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"encoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ latent_vector (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m50,240\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ latent_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,240\u001b[0m (196.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> (196.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,240\u001b[0m (196.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> (196.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ decoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            â”‚        \u001b[38;5;34m50,960\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ reshape (\u001b[38;5;33mReshape\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ decoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,960</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,960\u001b[0m (199.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,960</span> (199.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,960\u001b[0m (199.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,960</span> (199.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que tenemos el **encoder** (codificador) y el **decoder** (decodificador), los conectamos para formar el modelo **autoencoder completo**.  \n",
        "El flujo de trabajo es simple:\n",
        "\n",
        "> **Entrada (imagen 28x28)** â†’ **Encoder (comprime a 64 valores)** â†’ **Decoder (reconstruye la imagen 28x28)**\n",
        "\n",
        "De este modo, el autoencoder recibe una imagen, la **reduce** a su versiÃ³n mÃ¡s importante (64 nÃºmeros), y luego intenta **reconstruirla** a partir de esa informaciÃ³n comprimida.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "El resumen (`summary()`) muestra las **capas del encoder y del decoder** junto con su forma de salida y cuÃ¡ntos parÃ¡metros tienen.\n",
        "\n",
        "- En el **encoder**, la imagen (28x28 = 784 pÃ­xeles) pasa por:\n",
        "  - `Flatten`: aplana la imagen en un vector de 784 valores (sin parÃ¡metros).  \n",
        "  - `Dense (64)`: la comprime en **64 valores** â†’ este es el **vector latente**.  \n",
        "  - Tiene **50,240 parÃ¡metros entrenables**, que son los pesos que el modelo ajusta para aprender la mejor compresiÃ³n.\n",
        "\n",
        "- En el **decoder**, el proceso es inverso:\n",
        "  - `Input (64)`: recibe el vector latente.  \n",
        "  - `Dense (784)`: reconstruye la imagen completa de 28x28 pÃ­xeles.  \n",
        "  - `Reshape`: vuelve a darle forma de imagen.  \n",
        "  - Tiene **50,960 parÃ¡metros entrenables**, que aprende a partir de los datos para hacer buenas reconstrucciones.\n",
        "\n",
        "En total, el autoencoder entrena alrededor de **100 mil parÃ¡metros** â€”un modelo liviano pero suficiente para aprender las formas bÃ¡sicas de las prendas de Fashion-MNIST.\n",
        "\n"
      ],
      "metadata": {
        "id": "KvOHGdOprQYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crearmos el modelo autoencoder completo\n",
        "\n",
        "Ahora unimos ambos en un modelo end-to-end. BÃ¡sicamente alimentamos la entrada al encoder y luego el encoder al decoder. Podemos hacerlo definiendo un nuevo Input y conectando:"
      ],
      "metadata": {
        "id": "c72iD-Axrm7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo autoencoder uniendo encoder y decoder\n",
        "autoencoder_input = Input(shape=input_shape, name=\"ae_input\")\n",
        "encoded_latent = encoder_model(autoencoder_input)\n",
        "reconstructed_output = decoder_model(encoded_latent)\n",
        "autoencoder_model = Model(autoencoder_input, reconstructed_output, name=\"autoencoder\")\n",
        "print(autoencoder_model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "LhBw120sroSj",
        "outputId": "b908117e-8b50-4e95-c628-386222a5c6c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"autoencoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"autoencoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ ae_input (\u001b[38;5;33mInputLayer\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder (\u001b[38;5;33mFunctional\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m50,240\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder (\u001b[38;5;33mFunctional\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         â”‚        \u001b[38;5;34m50,960\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ ae_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,960</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,200\u001b[0m (395.31 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,200</span> (395.31 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,200\u001b[0m (395.31 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,200</span> (395.31 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El `summary()` del autoencoder muestra cÃ³mo los datos viajan a travÃ©s del modelo:  \n",
        "primero se **comprimen** (de 28x28 â†’ 64) y luego se **reconstruyen** (de 64 â†’ 28x28).  \n",
        "En total hay alrededor de **100 mil parÃ¡metros entrenables**, lo que significa que el modelo tiene esa cantidad de â€œajustes internosâ€ que irÃ¡ aprendiendo durante el entrenamiento.\n",
        "\n",
        "En este punto tenemos **tres modelos activos en memoria**:\n",
        "\n",
        "- **autoencoder_model:**  \n",
        "  Es el modelo completo. Recibe una imagen, la codifica, y luego la reconstruye.  \n",
        "  (Sirve para entrenar y evaluar el rendimiento total.)\n",
        "\n",
        "- **encoder_model:**  \n",
        "  Solo realiza la primera mitad del proceso.  \n",
        "  Toma una imagen y la transforma en su **vector latente de 64 valores** â€”es decir, su versiÃ³n comprimida.  \n",
        "  Esta parte aprende quÃ© caracterÃ­sticas son mÃ¡s importantes de cada prenda (forma, textura, contorno, etc.).\n",
        "\n",
        "- **decoder_model:**  \n",
        "  Hace la segunda mitad.  \n",
        "  Toma ese vector latente de 64 valores y lo convierte de nuevo en una imagen de 28x28 pÃ­xeles.  \n",
        "  BÃ¡sicamente â€œreconstruyeâ€ lo que el encoder comprimiÃ³.\n"
      ],
      "metadata": {
        "id": "4d8CmCSdr0MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compilar, entrenar y presentar resultados del modelo completo##"
      ],
      "metadata": {
        "id": "N4qsthFo_A66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que el modelo completo estÃ¡ listo, debemos **compilarlo** antes de entrenarlo.  \n",
        "Compilar significa definir **cÃ³mo** aprenderÃ¡ (optimizador) y **quÃ©** intentarÃ¡ minimizar (funciÃ³n de pÃ©rdida).\n",
        "\n",
        "- ğŸ”¹ **Optimizador:**  \n",
        "  Usaremos **Adam**, un algoritmo muy eficiente y estable que ajusta los parÃ¡metros automÃ¡ticamente mientras aprende.  \n",
        "  Es la opciÃ³n mÃ¡s comÃºn y confiable para redes neuronales modernas.\n",
        "\n",
        "- ğŸ”¹ **FunciÃ³n de pÃ©rdida:**  \n",
        "  Usaremos **MSE (Mean Squared Error)**, que mide el **error promedio entre los pÃ­xeles originales y los reconstruidos**.  \n",
        "  Cuanto mÃ¡s pequeÃ±o sea el MSE, mejor estarÃ¡ reconstruyendo el autoencoder.\n",
        "\n",
        "\n",
        "**MSE** es mÃ¡s intuitiva aquÃ­: simplemente mide cuÃ¡nta diferencia hay entre la imagen original y la reconstruida, pÃ­xel por pÃ­xel.\n",
        "\n",
        "##**Luego entrenamos(model.fit)**##.\n",
        " usando los datos de entrenamiento tanto como entrada como salida (puesto que la salida deseada es la misma imagen de entrada). Validaremos con el conjunto de prueba para monitorear el desempeÃ±o en datos no vistos."
      ],
      "metadata": {
        "id": "qTge77Z4-RcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Luego entrenamos**(model.fit) usando los datos de entrenamiento tanto como entrada como salida (puesto que la salida deseada es la misma imagen de entrada). Validaremos con el conjunto de prueba para monitorear el desempeÃ±o en datos no vistos."
      ],
      "metadata": {
        "id": "6CDpKDfssISr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar el autoencoder\n",
        "autoencoder_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Entrenar el modelo autoencoder\n",
        "epochs = 20\n",
        "batch_size = 256\n",
        "history = autoencoder_model.fit(\n",
        "    x_train, x_train,  # x (entrada) y y (salida) son el mismo conjunto de imÃ¡genes\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_test, x_test)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmMO9iZRsUf-",
        "outputId": "368ab08a-f2bc-4619-913e-f1a27ad16d10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0812 - val_loss: 0.0286\n",
            "Epoch 2/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0261 - val_loss: 0.0205\n",
            "Epoch 3/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0193 - val_loss: 0.0163\n",
            "Epoch 4/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0156 - val_loss: 0.0141\n",
            "Epoch 5/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0137 - val_loss: 0.0128\n",
            "Epoch 6/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0125 - val_loss: 0.0121\n",
            "Epoch 7/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0118 - val_loss: 0.0114\n",
            "Epoch 8/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0113 - val_loss: 0.0110\n",
            "Epoch 9/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0108 - val_loss: 0.0106\n",
            "Epoch 10/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0105 - val_loss: 0.0103\n",
            "Epoch 11/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 12/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0099 - val_loss: 0.0098\n",
            "Epoch 13/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0097 - val_loss: 0.0096\n",
            "Epoch 14/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0096 - val_loss: 0.0095\n",
            "Epoch 15/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0094 - val_loss: 0.0094\n",
            "Epoch 16/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0092 - val_loss: 0.0093\n",
            "Epoch 17/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0092 - val_loss: 0.0093\n",
            "Epoch 18/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0091 - val_loss: 0.0092\n",
            "Epoch 19/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0090 - val_loss: 0.0092\n",
            "Epoch 20/20\n",
            "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0089 - val_loss: 0.0090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante el entrenamiento del autoencoder, el modelo aprende comparando las imÃ¡genes originales con las reconstruidas.  \n",
        "En cada paso, calcula quÃ© tan diferentes son (usando el error MSE) y ajusta sus parÃ¡metros para mejorar la reconstrucciÃ³n.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ”¹ **Ã©poca (epoch)**\n",
        "\n",
        "Una *Ã©poca* significa que el modelo ha pasado **una vez completa por todos los datos de entrenamiento**.  \n",
        "En este caso, tenemos 60,000 imÃ¡genes de ropa (del dataset Fashion-MNIST).  \n",
        "Si entrenamos por 20 Ã©pocas, el modelo verÃ¡ **esas 60,000 imÃ¡genes 20 veces**, mejorando un poco mÃ¡s cada vez.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ”¹ **Â¿Por quÃ© 20 Ã©pocas?**\n",
        "\n",
        "- Es un nÃºmero razonable para que el modelo aprenda sin tardar demasiado.  \n",
        "- Con menos (por ejemplo 5), no llegarÃ­a a reconstruir bien las imÃ¡genes.  \n",
        "- Con muchas mÃ¡s (por ejemplo 100), podrÃ­a sobreajustarse, es decir, aprender solo a memorizar las imÃ¡genes de entrenamiento.  \n",
        "- Por eso 20 es un buen equilibrio: se observa que la pÃ©rdida (`loss`) baja y se estabiliza hacia el final.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ”¹ **â€œ235/235â€**\n",
        "\n",
        "El entrenamiento no se hace con las 60,000 imÃ¡genes todas a la vez, sino por **lotes pequeÃ±os (batches)**.  \n",
        "Cada lote contiene **256 imÃ¡genes** (definido por `batch_size = 256`).  \n",
        "Entonces, para recorrer las 60,000 imÃ¡genes, se necesitan unos **235 lotes (o pasos)**:\n",
        "  \n",
        "\\[\n",
        "60,000 \\div 256 â‰ˆ 235\n",
        "\\]\n",
        "\n",
        "Cada lÃ­nea `Epoch 1/20 â€” 235/235` significa que el modelo ya completÃ³ **los 235 pasos** para esa Ã©poca.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ”¹ **`loss` y `val_loss`?**\n",
        "\n",
        "- `loss`: el error promedio (MSE) con las imÃ¡genes de **entrenamiento**.  \n",
        "- `val_loss`: el error promedio con las imÃ¡genes de **validaciÃ³n (prueba)**, que el modelo no ha visto durante el entrenamiento.\n",
        "\n",
        "Si ambos valores bajan de forma pareja, significa que el modelo estÃ¡ aprendiendo correctamente.  \n",
        "En tu caso, comenzÃ³ con una `loss` de 0.08 y bajÃ³ hasta ~0.009, lo cual es excelente.  \n",
        "Eso indica que las imÃ¡genes reconstruidas se parecen mucho a las originales.\n"
      ],
      "metadata": {
        "id": "nnlAKAFlsg8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AquÃ­ optaremos por una visualizaciÃ³n de algunas imÃ¡genes reconstruidas, que es muy ilustrativa. Tomaremos algunas muestras del set de prueba, las pasaremos por el autoencoder, y compararemos con las originales."
      ],
      "metadata": {
        "id": "YkA3Ja9etM5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Codificar y reconstruir algunas imÃ¡genes de prueba\n",
        "n_muestras = 5\n",
        "# Tomamos n_muestras imÃ¡genes originales del conjunto de prueba\n",
        "muestras_originales = x_test[:n_muestras]\n",
        "# Las pasamos por el autoencoder para obtener sus reconstrucciones\n",
        "reconstrucciones = autoencoder_model.predict(muestras_originales)\n",
        "\n",
        "# Graficar las imÃ¡genes originales vs reconstruidas\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(n_muestras):\n",
        "    # Imagen original\n",
        "    ax = plt.subplot(2, n_muestras, i+1)\n",
        "    plt.imshow(muestras_originales[i], cmap='gray')\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "    # Imagen reconstruida\n",
        "    ax = plt.subplot(2, n_muestras, i+1+n_muestras)\n",
        "    plt.imshow(reconstrucciones[i], cmap='gray')\n",
        "    plt.title(\"Reconstruido\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "kN96cTRatQGu",
        "outputId": "98247b33-8d7f-40bf-95ab-86221160120b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este cÃ³digo mostrarÃ¡ una figura con dos filas: la fila superior son 5 imÃ¡genes originales de prueba, y la fila inferior son las correspondientes imÃ¡genes reconstruidas por el autoencoder.\n",
        "\n",
        "Ejemplo de reconstrucciÃ³n de imÃ¡genes del conjunto Fashion-MNIST usando un autoencoder simple. En la fila superior estÃ¡n las imÃ¡genes de entrada originales (prendas de ropa) y en la inferior las imÃ¡genes reconstruidas por el autoencoder. Se aprecia que el modelo logra capturar la forma general de los objetos, aunque pierde algunos detalles finos.\n",
        "\n",
        "Como podemos observar, las reconstrucciones son razonablemente similares a los originales. Por ejemplo, si la imagen original era un zapato o una camisa, la reconstrucciÃ³n mantiene la silueta y proporciones generales. Esto indica que la representaciÃ³n latente de 64 dimensiones ha logrado codificar suficiente informaciÃ³n relevante de la imagen\n",
        "meegle.com\n",
        ". El error de reconstrucciÃ³n numÃ©rico (MSE) suele estar por debajo de 0.02 para estas imÃ¡genes normalizadas, aunque ese valor exacto dependerÃ¡ de las Ã©pocas entrenadas y la arquitectura."
      ],
      "metadata": {
        "id": "6qmVWbDrtQ8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos tambiÃ©n inspeccionar directamente la salida del codificador. Por ejemplo:"
      ],
      "metadata": {
        "id": "GQXlPSTstotJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener representaciÃ³n latente de una imagen\n",
        "latente = encoder_model.predict(muestras_originales[:1])\n",
        "print(\"Vector latente de la primera imagen:\", latente)\n",
        "print(\"DimensiÃ³n del vector latente:\", latente.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J66xKoPdtqp1",
        "outputId": "38cf72f0-e1f7-464a-a4d5-25e6715e7d1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Vector latente de la primera imagen: [[ 4.606557   2.2451649  1.3315871  1.1544847  7.486549   1.319545\n",
            "   5.485861   2.4805644  3.7037036  2.2479022  5.7322946  1.2009276\n",
            "   1.6585429  1.8976891  4.8914227  3.2431304  1.8614359  1.1921142\n",
            "   7.7437882  1.796972   4.902409   2.715697   3.7479515  4.161723\n",
            "   5.845678   2.8111567  1.1447109  2.7625952  4.3787236  1.4301591\n",
            "   8.418474   3.0563805  1.4712446  4.575574   2.0738988  5.5130234\n",
            "   2.9324715  4.6496863  4.3712053  3.991526   4.718972   5.226871\n",
            "   2.2924995  6.619038   5.672697   2.825334   2.8059676  1.277573\n",
            "   3.0546532  4.293191   1.7452891  1.7545468  2.051986   3.577681\n",
            "   7.0079937  2.257664   2.3891428  5.9038296  3.56917   10.550139\n",
            "   3.0608006  4.671793   4.220473   4.219145 ]]\n",
            "DimensiÃ³n del vector latente: (1, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto imprimirÃ­a el vector de 64 nÃºmeros que representa, digamos, una bota o una camiseta en el espacio comprimido. No es interpretable directamente sin tÃ©cnicas adicionales, pero podrÃ­amos usarlo, por ejemplo, para visualizar con reducciÃ³n a 2D (t-SNE, PCA) cÃ³mo quedan distribuidas las imÃ¡genes en el espacio latente, o alimentarlo a otro modelo."
      ],
      "metadata": {
        "id": "MWhtbLrdtuRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EvaluaciÃ³n final y siguientes pasos"
      ],
      "metadata": {
        "id": "N5lyf7PHuL1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cuanto a la evaluaciÃ³n cuantitativa, podrÃ­amos calcular el error medio de reconstrucciÃ³n en todo el conjunto de prueba:"
      ],
      "metadata": {
        "id": "VIXssIyVuM_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_test = autoencoder_model.evaluate(x_test, x_test, verbose=0)\n",
        "print(f\"Error MSE promedio en el conjunto de prueba: {mse_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S5OiBL6uTs-",
        "outputId": "bcbf21aa-e05e-4d19-89e1-4d387647588c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error MSE promedio en el conjunto de prueba: 0.0090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto nos da una idea de quÃ© tan bien comprime y reconstruye el autoencoder en general. Un valor cercano al obtenido en entrenamiento indicarÃ­a que no hubo sobreajuste significativo (las pÃ©rdidas de entren/val eran similares).\n",
        "\n",
        "##Para mejorar fine-tuning, podrÃ­amos intentar mejorar este autoencoder:\n",
        "\n",
        "AÃ±adiendo mÃ¡s neuronas o capas (por ejemplo, hacer el encoder con dos capas densas: 128 -> 64).\n",
        "\n",
        "Probando una arquitectura convolucional, que suele capturar mejor la estructura de imÃ¡genes (ejemplo: conv layers en encoder para bajar a 7x7x8 = 392 latent, y conv-transpose en decoder). Eso probablemente darÃ­a reconstrucciones mÃ¡s nÃ­tidas.\n",
        "\n",
        "Entrenando mÃ¡s Ã©pocas hasta que la pÃ©rdida prÃ¡cticamente no mejore.Para mejorar este autoencoder en el futuro, podrÃ­amos probar:\n",
        "\n",
        "- **MÃ¡s capas o neuronas:** por ejemplo, usar un encoder con dos capas densas (128 â†’ 64) para captar mÃ¡s detalle.  \n",
        "- **MÃ¡s Ã©pocas:** seguir entrenando hasta que la pÃ©rdida deje de mejorar.  \n",
        "- **Denoising autoencoder:** aÃ±adir ruido a las imÃ¡genes de entrada y pedir al modelo que las reconstruya limpias, haciÃ©ndolo mÃ¡s robusto.  \n",
        "- **Fine-tuning para clasificaciÃ³n:** congelar el encoder y agregar una capa *softmax* para clasificar las prendas usando los 64 valores latentes como caracterÃ­sticas."
      ],
      "metadata": {
        "id": "4rkDnrW9uWcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto, hemos cubierto todo lo pedido:\n",
        "- 1.- carga de datos.\n",
        "- 2.-definiciÃ³n de dimensiones.\n",
        "- 3.-construcciÃ³n del encoder/decoder.\n",
        "- 4.- armado del modelo autoencoder y del encoder por separado.\n",
        "- 5.-entrenamiento y visualizaciÃ³n de resultados."
      ],
      "metadata": {
        "id": "Hdfs1Z-Numby"
      }
    }
  ]
}