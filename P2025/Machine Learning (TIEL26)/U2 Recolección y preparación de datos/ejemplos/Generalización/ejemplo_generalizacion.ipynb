{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de Generalización vs. Overfitting\n",
    "\n",
    "La **generalización** es la capacidad de un modelo para funcionar bien con datos nuevos que no ha visto durante el entrenamiento. Un modelo que generaliza bien ha aprendido los patrones subyacentes en los datos, en lugar de simplemente memorizar el ruido.\n",
    "\n",
    "- **Buen modelo (Generalización):** Captura la tendencia general de los datos.\n",
    "- **Mal modelo (Overfitting):** Se ajusta demasiado a los datos de entrenamiento, incluido el ruido, y no funciona bien con datos nuevos.\n",
    "\n",
    "En este ejemplo, compararemos un modelo de regresión polinómica simple con uno complejo para ilustrar este concepto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generar Datos Sintéticos\n",
    "\n",
    "Crearemos datos que siguen una función coseno, pero con algo de ruido añadido para simular un conjunto de datos real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función verdadera (coseno)\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# Generar datos de entrenamiento con ruido\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "X_train = np.sort(np.random.rand(n_samples))\n",
    "y_train = true_fun(X_train) + np.random.randn(n_samples) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Entrenar Dos Modelos Diferentes\n",
    "\n",
    "- **Modelo 1 (Grado 4):** Un polinomio de grado 4, que debería ser suficiente para capturar la forma de la curva del coseno.\n",
    "- **Modelo 2 (Grado 15):** Un polinomio de grado 15, que es demasiado complejo y propenso a sobreajustar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grados de los polinomios a comparar\n",
    "degrees = [4, 15]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    # Crear y entrenar el pipeline del modelo\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = make_pipeline(polynomial_features, linear_regression)\n",
    "    pipeline.fit(X_train[:, np.newaxis], y_train)\n",
    "\n",
    "    # Puntos para la predicción (línea continua)\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    y_pred = pipeline.predict(X_test[:, np.newaxis])\n",
    "\n",
    "    # Graficar resultados\n",
    "    plt.plot(X_test, y_pred, label=\"Modelo Predicho\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"Función Verdadera\")\n",
    "    plt.scatter(X_train, y_train, edgecolor='b', s=20, label=\"Muestras de Entrenamiento\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(f\"Grado {degree} (Overfitting)\" if degree > 10 else f\"Grado {degree} (Buen Ajuste)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "- El **modelo de grado 4** (izquierda) se parece mucho a la función verdadera. Ha aprendido el patrón subyacente y, por lo tanto, **generaliza bien**.\n",
    "- El **modelo de grado 15** (derecha) pasa exactamente por casi todos los puntos de entrenamiento, pero su forma es extremadamente compleja y errática. Este modelo ha memorizado el ruido y no servirá para predecir nuevos puntos. Esto es un claro ejemplo de **overfitting**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
