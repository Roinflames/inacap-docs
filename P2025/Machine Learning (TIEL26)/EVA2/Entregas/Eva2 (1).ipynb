{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5120fc99"
      },
      "source": [
        "# Task\n",
        "Re-implement the SHIA pipeline from scratch in the notebook, ensuring it is correctly structured and includes steps for data loading, preprocessing, model training (base models with Dask, meta-learners, and a final blender), generating meta-features, simulating incremental data arrival, incremental training of the blender, evaluation, saving models, and logging results with MLflow. The pipeline should utilize Spark and Dask for distributed processing and parallel training where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "6fa1e1ff",
        "outputId": "421d96a9-366a-4c00-8c8b-4f49ed4894b0"
      },
      "source": [
        "# ----------------------------\n",
        "# IMPORTS\n",
        "# ----------------------------\n",
        "# !pip install pyspark dask distributed pandas scikit-learn joblib mlflow requests --quiet\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Spark\n",
        "# from pyspark.sql import SparkSession # Not needed in this subtask\n",
        "\n",
        "# Dask\n",
        "# from dask.distributed import Client, LocalCluster # Not needed in this subtask\n",
        "\n",
        "# ML\n",
        "# from sklearn.model_selection import train_test_split # Not needed in this subtask\n",
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Not needed in this subtask\n",
        "# from sklearn.svm import SVC # Not needed in this subtask\n",
        "# from sklearn.linear_model import LogisticRegression # Not needed in this subtask\n",
        "# from sklearn.metrics import accuracy_score, f1_score # Not needed in this subtask\n",
        "# from sklearn.preprocessing import StandardScaler # Not needed in this subtask\n",
        "\n",
        "# Stacking / SHIA-level\n",
        "# from sklearn.ensemble import StackingClassifier # Not needed in this subtask\n",
        "# from xgboost import XGBClassifier # Not needed in this subtask\n",
        "\n",
        "import mlflow\n",
        "\n",
        "# Repro\n",
        "RND = 42\n",
        "np.random.seed(RND)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Descargar CSV localmente (Spark no lee https directamente)\n",
        "# ----------------------------\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "local_path = \"/tmp/titanic.csv\"\n",
        "os.makedirs(\"/tmp\", exist_ok=True)\n",
        "\n",
        "if not os.path.exists(local_path):\n",
        "    print(\"Descargando dataset:\", url)\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "    print(\"Descarga completa ->\", local_path)\n",
        "else:\n",
        "    print(\"Archivo ya existe:\", local_path)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Cargar con Pandas (inspección rápida)\n",
        "# ----------------------------\n",
        "pdf = pd.read_csv(local_path)\n",
        "print(\"Pandas df shape:\", pdf.shape)\n",
        "display(pdf.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo ya existe: /tmp/titanic.csv\n",
            "Pandas df shape: (891, 12)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe4142fa-d6f4-4a30-98fa-5697a47dd32b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe4142fa-d6f4-4a30-98fa-5697a47dd32b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe4142fa-d6f4-4a30-98fa-5697a47dd32b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe4142fa-d6f4-4a30-98fa-5697a47dd32b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8320797f-58e3-458a-8720-dcc8ba472cb0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8320797f-58e3-458a-8720-dcc8ba472cb0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8320797f-58e3-458a-8720-dcc8ba472cb0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pdf\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"PassengerId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Braund, Mr. Owen Harris\",\n          \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.32666399786453,\n        \"min\": 22.0,\n        \"max\": 38.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          22.0,\n          38.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ticket\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A/5 21171\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.77633594396447,\n        \"min\": 7.25,\n        \"max\": 71.2833,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          7.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cabin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"C85\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c76943"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a Spark session and convert the Pandas DataFrame to a Spark DataFrame for potential future distributed processing steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e6ffd9b",
        "outputId": "623e279f-494a-4e1b-8aff-139d650d9c2f"
      },
      "source": [
        "# ----------------------------\n",
        "# 3) Inicializar Spark\n",
        "# ----------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SHIA_Enterprise_Hybrid\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "print(\"Spark versión:\", spark.version)\n",
        "\n",
        "# Convertir Pandas -> Spark DataFrame\n",
        "spark_df = spark.createDataFrame(pdf)\n",
        "print(\"Spark DF creado:\")\n",
        "spark_df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark versión: 3.5.1\n",
            "Spark DF creado:\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|  NaN|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|  NaN|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9feb8fe7"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the Pandas DataFrame to a Dask DataFrame for potential future parallel processing steps with Dask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34564540",
        "outputId": "f71b3fde-dcb7-46af-8ddd-9fc04d7e3363"
      },
      "source": [
        "# ----------------------------\n",
        "# 4) Convertir a Dask DataFrame (opcional)\n",
        "# ----------------------------\n",
        "import dask.dataframe as dd\n",
        "ddf = dd.from_pandas(pdf, npartitions=4)\n",
        "print(\"Dask DataFrame particionado en:\", ddf.npartitions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask DataFrame particionado en: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a4afc97"
      },
      "source": [
        "**Reasoning**:\n",
        "Perform basic preprocessing steps on the Spark DataFrame (filling missing values and encoding 'Sex') and then convert the cleaned Spark DataFrame back to a Pandas DataFrame for subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e130f9c5",
        "outputId": "b6d69fa1-3ba3-4623-b3ce-e9c03b8863a8"
      },
      "source": [
        "# ----------------------------\n",
        "# 5) Preprocesamiento (Spark/Pandas)\n",
        "#    - Rellenar nulos, codificar sexo, seleccionar features\n",
        "# ----------------------------\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Hacemos transformaciones en Spark (para simular pipeline a escala)\n",
        "spark_df = spark_df.fillna({'Age': 30, 'Embarked': 'S'})\n",
        "spark_df = spark_df.withColumn(\"Sex\", when(col(\"Sex\") == \"male\", 1).otherwise(0))\n",
        "\n",
        "# Traer a Pandas para el resto del flujo (meta/SHIA)\n",
        "pdf_clean = spark_df.toPandas()\n",
        "\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "target = \"Survived\"\n",
        "\n",
        "X = pdf_clean[features].copy()\n",
        "y = pdf_clean[target].copy()\n",
        "print(\"Features shape:\", X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (891, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3e5e01f"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data into training, validation, and test sets to prepare for model training and evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1564a311",
        "outputId": "1fa7b1fd-ea15-4267-a6ce-aa4292d08b9c"
      },
      "source": [
        "# ----------------------------\n",
        "# 6) Train/Val/Test split (Pandas)\n",
        "# ----------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_hold, y_train, y_hold = train_test_split(X, y, test_size=0.30, stratify=y, random_state=RND)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_hold, y_hold, test_size=0.5, stratify=y_hold, random_state=RND)\n",
        "print(\"Train/Val/Test shapes:\", X_train.shape, X_val.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val/Test shapes: (623, 6) (134, 6) (134, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d865e057"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a Dask LocalCluster and Client to set up the environment for parallel model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abf0c7ed",
        "outputId": "031d7cc1-3c8e-4810-fdae-bc05e14f25a5"
      },
      "source": [
        "# ----------------------------\n",
        "# 7) Iniciar Dask (LocalCluster)\n",
        "# ----------------------------\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit=\"2GB\")\n",
        "client = Client(cluster)\n",
        "print(\"Dask dashboard:\", client.dashboard_link)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 40623 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:35843\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:40623/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45777'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:32995'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44823'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36443'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:44493 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:44493\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:42024\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:37567 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37567\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:42042\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46855 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46855\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:42026\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:45019 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:45019\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:42048\n",
            "INFO:distributed.scheduler:Receive client connection: Client-d4edeb42-b065-11f0-80ea-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:42064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask dashboard: http://127.0.0.1:40623/status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6408fc29"
      },
      "source": [
        "**Reasoning**:\n",
        "Define and train the base models in parallel using Dask, then collect the trained models and save them locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u4dmwL6WRA2",
        "outputId": "0777d642-6739-4606-98dc-b83b84666259"
      },
      "source": [
        "# ----------------------------\n",
        "# 8) Entrenamiento de modelos base en paralelo con Dask\n",
        "#    - Función que entrena y devuelve el modelo (se recoge en el driver)\n",
        "# ----------------------------\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_model_return(name, model, X_tr, y_tr):\n",
        "    \"\"\"\n",
        "    Entrena un modelo (pandas DataFrame/Series) y devuelve el objeto ajustado.\n",
        "    \"\"\"\n",
        "    model.fit(X_tr, y_tr)\n",
        "    return (name, model)\n",
        "\n",
        "# modelos base\n",
        "base_models = {\n",
        "    \"M1_rf\": RandomForestClassifier(n_estimators=100, random_state=RND),\n",
        "    \"M2_gb\": GradientBoostingClassifier(random_state=RND),\n",
        "    \"M3_svc\": SVC(probability=True, kernel='rbf', C=2, random_state=RND),\n",
        "    \"M4_lr\": LogisticRegression(max_iter=500, random_state=RND)\n",
        "}\n",
        "\n",
        "# enviar tareas a Dask (los datos X_train,y_train se serializan hacia los workers)\n",
        "futures = []\n",
        "for name, model in base_models.items():\n",
        "    fut = client.submit(train_model_return, name, model, X_train, y_train)\n",
        "    futures.append(fut)\n",
        "\n",
        "# recoger resultados (modelos ajustados en driver)\n",
        "trained = client.gather(futures)\n",
        "models_trained = {name: mdl for name, mdl in trained}\n",
        "\n",
        "# Guardar localmente (driver) con joblib\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "for name, mdl in models_trained.items():\n",
        "    path = models_dir / f\"{name}.joblib\"\n",
        "    joblib.dump(mdl, path)\n",
        "    print(\"Guardado:\", path)\n",
        "\n",
        "print(\"Modelos base entrenados:\", list(models_trained.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardado: models/M1_rf.joblib\n",
            "Guardado: models/M2_gb.joblib\n",
            "Guardado: models/M3_svc.joblib\n",
            "Guardado: models/M4_lr.joblib\n",
            "Modelos base entrenados: ['M1_rf', 'M2_gb', 'M3_svc', 'M4_lr']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79f53df5"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate out-of-fold predictions for the training set using cross-validation with the trained base models to create meta-features for the meta-learners.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42476795",
        "outputId": "d104cda2-0bbc-439b-bd35-da46b9a4193a"
      },
      "source": [
        "# ----------------------------\n",
        "# 9) Generar meta-features con validación cruzada (para entrenamiento del meta-learner)\n",
        "# ----------------------------\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RND)\n",
        "\n",
        "# Dictionary to store out-of-fold predictions\n",
        "meta_train_oof = {}\n",
        "\n",
        "# Iterate through base models\n",
        "for name, model in models_trained.items():\n",
        "    print(f\"Generating OOF predictions for {name}...\")\n",
        "    # Array to store OOF predictions for the current model\n",
        "    oof_preds = np.zeros(X_train.shape[0])\n",
        "\n",
        "    # Iterate through cross-validation splits\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "        # Create a copy of the model to train on the current fold\n",
        "        model_copy = model.__class__(**model.get_params())\n",
        "\n",
        "        # Train the model copy on the training fold\n",
        "        model_copy.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
        "\n",
        "        # Make predictions (probabilities for the positive class) on the validation fold\n",
        "        # Check if the model has predict_proba, otherwise use predict\n",
        "        if hasattr(model_copy, 'predict_proba'):\n",
        "            predictions = model_copy.predict_proba(X_train.iloc[val_idx])[:, 1]\n",
        "        else:\n",
        "            predictions = model_copy.predict(X_train.iloc[val_idx])\n",
        "\n",
        "        # Store predictions in the OOF array\n",
        "        oof_preds[val_idx] = predictions\n",
        "\n",
        "    # Store OOF predictions for the current model\n",
        "    meta_train_oof[name] = oof_preds\n",
        "    print(f\"Finished OOF predictions for {name}.\")\n",
        "\n",
        "print(\"\\nMeta-features (OOF predictions) generated:\", meta_train_oof.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating OOF predictions for M1_rf...\n",
            "Finished OOF predictions for M1_rf.\n",
            "Generating OOF predictions for M2_gb...\n",
            "Finished OOF predictions for M2_gb.\n",
            "Generating OOF predictions for M3_svc...\n",
            "Finished OOF predictions for M3_svc.\n",
            "Generating OOF predictions for M4_lr...\n",
            "Finished OOF predictions for M4_lr.\n",
            "\n",
            "Meta-features (OOF predictions) generated: dict_keys(['M1_rf', 'M2_gb', 'M3_svc', 'M4_lr'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ba71e0"
      },
      "source": [
        "## Generación de meta-features (validación y prueba)\n",
        "\n",
        "### Subtask:\n",
        "Generar las meta-features para los conjuntos de validación (`X_val`) y prueba (`X_test`) utilizando las predicciones de los modelos base *completamente entrenados* (en todo el conjunto de entrenamiento, disponibles en `models_trained`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad9e138"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the trained base models, predict probabilities on the validation and test sets, and store these predictions in dictionaries. Then, stack these predictions into NumPy arrays and print their shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72f506c6",
        "outputId": "87b039e9-76c9-4da0-9e05-a688757f7594"
      },
      "source": [
        "# Iterate through fully trained base models\n",
        "meta_val_preds = {}\n",
        "meta_test_preds = {}\n",
        "\n",
        "for name, model in models_trained.items():\n",
        "    print(f\"Generating meta-features for validation and test sets using {name}...\")\n",
        "    # Predict probabilities on validation set\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        meta_val_preds[name] = model.predict_proba(X_val)[:, 1]\n",
        "        meta_test_preds[name] = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        meta_val_preds[name] = model.predict(X_val)\n",
        "        meta_test_preds[name] = model.predict(X_test)\n",
        "    print(f\"Finished meta-feature generation for {name}.\")\n",
        "\n",
        "# Stack the meta-features into NumPy arrays\n",
        "meta_val = np.column_stack(list(meta_val_preds.values()))\n",
        "meta_test = np.column_stack(list(meta_test_preds.values()))\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"\\nShape of stacked meta_val:\", meta_val.shape)\n",
        "print(\"Shape of stacked meta_test:\", meta_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating meta-features for validation and test sets using M1_rf...\n",
            "Finished meta-feature generation for M1_rf.\n",
            "Generating meta-features for validation and test sets using M2_gb...\n",
            "Finished meta-feature generation for M2_gb.\n",
            "Generating meta-features for validation and test sets using M3_svc...\n",
            "Finished meta-feature generation for M3_svc.\n",
            "Generating meta-features for validation and test sets using M4_lr...\n",
            "Finished meta-feature generation for M4_lr.\n",
            "\n",
            "Shape of stacked meta_val: (134, 4)\n",
            "Shape of stacked meta_test: (134, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f867948d"
      },
      "source": [
        "## Entrenamiento de meta-learners (h1 y h2)\n",
        "\n",
        "### Subtask:\n",
        "Entrenar los modelos H1 y H2 utilizando las meta-features generadas a partir de la validación cruzada en el conjunto de entrenamiento (`meta_train_oof`) y el target de entrenamiento (`y_train`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e51d960b"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the meta-learners H1 and H2 using the generated meta-features from the training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7b8596e",
        "outputId": "f78a6af7-fa2c-49c7-87c1-3b222213d74e"
      },
      "source": [
        "# ----------------------------\n",
        "# 9) Entrenar meta-learners H1 y H2 (Pandas/scikit-learn)\n",
        "#    - Usar meta-features OOF (meta_train) y targets (y_train)\n",
        "# ----------------------------\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Ensure meta_train is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_train_oof, dict):\n",
        "    # If meta_train is still a dictionary of arrays, stack it\n",
        "    meta_train = np.column_stack(list(meta_train_oof.values()))\n",
        "else:\n",
        "    # If it's already a numpy array, just use it\n",
        "    meta_train = meta_train_oof\n",
        "\n",
        "\n",
        "mid = max(1, meta_train.shape[1] // 2)\n",
        "print(f\"Dividing meta-features at column index: {mid}\")\n",
        "\n",
        "# modelos meta\n",
        "H1 = GradientBoostingClassifier(random_state=RND)\n",
        "H2 = LogisticRegression(max_iter=1000, random_state=RND)\n",
        "\n",
        "# Para entrenar H1/H2 usamos las meta-features generadas con OOF (meta_train) y y_train\n",
        "# Ensure y_train is a numpy array or pandas Series\n",
        "if isinstance(y_train, pd.Series):\n",
        "    y_train_np = y_train.values\n",
        "else:\n",
        "    y_train_np = y_train\n",
        "\n",
        "print(\"Training H1...\")\n",
        "H1.fit(meta_train[:, :mid], y_train_np)\n",
        "print(\"H1 trained.\")\n",
        "\n",
        "print(\"Training H2...\")\n",
        "H2.fit(meta_train[:, mid:], y_train_np)\n",
        "print(\"H2 trained.\")\n",
        "\n",
        "print(\"Meta-learners H1 and H2 trained.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing meta-features at column index: 2\n",
            "Training H1...\n",
            "H1 trained.\n",
            "Training H2...\n",
            "H2 trained.\n",
            "Meta-learners H1 and H2 trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94530e6e"
      },
      "source": [
        "## Entrenamiento del blender final (inicial)\n",
        "\n",
        "### Subtask:\n",
        "Entrenar el modelo `StackingClassifier` utilizando las predicciones probabilísticas de H1 y H2 sobre el conjunto de validación (`meta_val`) como features de entrada, y el target de validación (`y_val`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cdfb09"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate and fit the StackingClassifier using the predictions of H1 and H2 on the validation set as input features and the validation target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b765b4c6",
        "outputId": "e4e3cba8-4a9e-47fe-ae6a-8a38192d121c"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# ----------------------------\n",
        "# 10) Entrenar blender final Hf (inicial)\n",
        "# ----------------------------\n",
        "\n",
        "# features for Hf (outputs probabilísticos de H1/H2 sobre validación)\n",
        "# Ensure meta_val is a numpy array\n",
        "if isinstance(meta_val, dict):\n",
        "    # If meta_val is still a dictionary of arrays, stack it\n",
        "    meta_val = np.column_stack(list(meta_val.values()))\n",
        "\n",
        "# Calculate the mid point based on the number of columns in meta_val\n",
        "mid = max(1, meta_val.shape[1] // 2)\n",
        "\n",
        "# Ensure y_val is a numpy array or pandas Series\n",
        "if isinstance(y_val, pd.Series):\n",
        "    y_val_np = y_val.values\n",
        "else:\n",
        "    y_val_np = y_val\n",
        "\n",
        "# Generate the input features for the final blender on the validation set\n",
        "meta_for_Hf_val = np.column_stack([\n",
        "    H1.predict_proba(meta_val[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_val[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "# Final blender Hf: StackingClassifier with H1/H2 as estimators\n",
        "final_blender = StackingClassifier(\n",
        "    estimators=[('h1', H1), ('h2', H2)],\n",
        "    final_estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RND),\n",
        "    cv=3,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "print(\"Training final blender (StackingClassifier)...\")\n",
        "# Fit the StackingClassifier to the meta_for_Hf_val (predictions of H1/H2 on validation) and y_val\n",
        "final_blender.fit(meta_for_Hf_val, y_val_np)\n",
        "print(\"Final blender (StackingClassifier) trained.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final blender (StackingClassifier)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [23:14:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final blender (StackingClassifier) trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db42fb67"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained StackingClassifier on the test set using the predictions of H1 and H2 on meta_test as input features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXZTEATHWovM",
        "outputId": "77c8e77c-1537-4301-adbd-64d8cbb80db1"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Ensure meta_test is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_test, dict):\n",
        "    # If meta_test is still a dictionary of arrays, stack it\n",
        "    meta_test = np.column_stack(list(meta_test.values()))\n",
        "\n",
        "# Calculate the mid point based on the number of columns in meta_test\n",
        "mid = max(1, meta_test.shape[1] // 2)\n",
        "\n",
        "# Generate the input features for the final blender on the test set\n",
        "meta_for_Hf_test = np.column_stack([\n",
        "    H1.predict_proba(meta_test[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_test[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "# Use the trained final_blender model to make predictions on the test set meta-features\n",
        "y_pred_test = final_blender.predict(meta_for_Hf_test)\n",
        "\n",
        "# Ensure y_test is a numpy array or pandas Series\n",
        "if isinstance(y_test, pd.Series):\n",
        "    y_test_np = y_test.values\n",
        "else:\n",
        "    y_test_np = y_test\n",
        "\n",
        "# Calculate the accuracy score\n",
        "acc = accuracy_score(y_test_np, y_pred_test)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test_np, y_pred_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"SHIA Final - Test Accuracy:\", acc, \"F1:\", f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIA Final - Test Accuracy: 0.7164179104477612 F1: 0.6274509803921569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aedbfe3a"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate a synthetic dataset by adding noise to a subset of the training data, create a corresponding target variable by sampling from the training target, and generate meta-features for the new data using the trained base models. Then, stack the meta-features into a NumPy array and print the shapes of the new data, target, and meta-features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e50e0750",
        "outputId": "577041ea-8c96-4d87-83f4-ea6b9c78dca9"
      },
      "source": [
        "# ----------------------------\n",
        "# 11) Simular llegada de nuevos datos (para entrenamiento incremental)\n",
        "#     - Generar datos sintéticos con ruido\n",
        "#     - Generar meta-features para los nuevos datos\n",
        "# ----------------------------\n",
        "\n",
        "# 1. Generate new synthetic dataset by adding random noise to a subset of the original training data\n",
        "new_data_size = int(0.20 * X_train.shape[0])\n",
        "X_new = X_train.sample(n=new_data_size, random_state=RND, replace=True).reset_index(drop=True)\n",
        "\n",
        "# Calculate standard deviation for noise column-wise\n",
        "std_dev = X_train.std()\n",
        "noise_level = 0.01\n",
        "noise = np.random.normal(0, noise_level * std_dev, size=X_new.shape)\n",
        "X_new = X_new + noise\n",
        "# Ensure numerical columns remain numerical after adding noise\n",
        "X_new = X_new[features]\n",
        "\n",
        "# 2. Create a corresponding target variable for the simulated data\n",
        "y_new = y_train.sample(n=new_data_size, random_state=RND, replace=True).reset_index(drop=True)\n",
        "\n",
        "# 3. Generate meta-features for the new data using the trained base models\n",
        "meta_new_preds = {}\n",
        "for name, model in models_trained.items():\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        meta_new_preds[name] = model.predict_proba(X_new)[:, 1]\n",
        "    else:\n",
        "        meta_new_preds[name] = model.predict(X_new)\n",
        "\n",
        "# 4. Stack the meta-features from meta_new_preds into a NumPy array\n",
        "meta_new = np.column_stack(list(meta_new_preds.values()))\n",
        "\n",
        "# 5. Print the shapes of X_new, y_new, and meta_new\n",
        "print(\"Shape of simulated new data (X_new):\", X_new.shape)\n",
        "print(\"Shape of simulated new target (y_new):\", y_new.shape)\n",
        "print(\"Shape of new meta-features (meta_new):\", meta_new.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of simulated new data (X_new): (124, 6)\n",
            "Shape of simulated new target (y_new): (124,)\n",
            "Shape of new meta-features (meta_new): (124, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9408a33"
      },
      "source": [
        "**Reasoning**:\n",
        "Concatenate the original validation meta-features and target with the new simulated data and then train a new StackingClassifier incrementally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566e914c",
        "outputId": "19cae43c-fb77-424c-f93c-13c115dc5151"
      },
      "source": [
        "# 1. Concatenate original validation meta-features and new simulated meta-features\n",
        "# Ensure meta_val is a numpy array\n",
        "if isinstance(meta_val, dict):\n",
        "    meta_val = np.column_stack(list(meta_val.values()))\n",
        "\n",
        "combined_meta = np.vstack([meta_val, meta_new])\n",
        "print(\"Shape of combined meta-features:\", combined_meta.shape)\n",
        "\n",
        "# 2. Concatenate original validation target and new simulated target\n",
        "# Ensure y_val and y_new are numpy arrays\n",
        "if isinstance(y_val, pd.Series):\n",
        "    y_val_np = y_val.values\n",
        "else:\n",
        "    y_val_np = y_val\n",
        "\n",
        "if isinstance(y_new, pd.Series):\n",
        "    y_new_np = y_new.values\n",
        "else:\n",
        "    y_new_np = y_new\n",
        "\n",
        "combined_y = np.concatenate([y_val_np, y_new_np])\n",
        "print(\"Shape of combined target:\", combined_y.shape)\n",
        "\n",
        "\n",
        "# 3. Instantiate a new StackingClassifier model (Hf_new)\n",
        "# Use the same configuration as the initial blender\n",
        "Hf_new = StackingClassifier(\n",
        "    estimators=[('h1', H1), ('h2', H2)],\n",
        "    final_estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RND),\n",
        "    cv=3,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "# 4. Train the new StackingClassifier (Hf_new) using the combined data\n",
        "print(\"Training new StackingClassifier (Hf_new) incrementally...\")\n",
        "# The meta-features for the final blender are the probabilities from H1 and H2.\n",
        "# Need to re-generate these probabilities on the combined_meta data.\n",
        "\n",
        "mid = max(1, combined_meta.shape[1] // 2)\n",
        "\n",
        "meta_for_Hf_combined = np.column_stack([\n",
        "    H1.predict_proba(combined_meta[:, :mid])[:, 1],\n",
        "    H2.predict_proba(combined_meta[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "Hf_new.fit(meta_for_Hf_combined, combined_y)\n",
        "print(\"New StackingClassifier (Hf_new) trained.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of combined meta-features: (258, 4)\n",
            "Shape of combined target: (258,)\n",
            "Training new StackingClassifier (Hf_new) incrementally...\n",
            "New StackingClassifier (Hf_new) trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [23:15:20] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b65ec299"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the incrementally trained StackingClassifier (Hf_new) on the test set using the specified steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scJdVCTcWx-Q",
        "outputId": "07e41f5c-d2ad-4134-a22e-aa0187381805"
      },
      "source": [
        "# Ensure meta_test is a numpy array or pandas DataFrame\n",
        "if isinstance(meta_test, dict):\n",
        "    # If meta_test is still a dictionary of arrays, stack it\n",
        "    meta_test = np.column_stack(list(meta_test.values()))\n",
        "\n",
        "# Calculate the mid point based on the number of columns in meta_test\n",
        "mid = max(1, meta_test.shape[1] // 2)\n",
        "\n",
        "# Generate the input features for the incremental blender on the test set\n",
        "meta_for_Hf_test = np.column_stack([\n",
        "    H1.predict_proba(meta_test[:, :mid])[:, 1],\n",
        "    H2.predict_proba(meta_test[:, mid:])[:, 1]\n",
        "])\n",
        "\n",
        "# Use the incrementally trained Hf_new model to make predictions on the test set meta-features\n",
        "y_pred_test_inc = Hf_new.predict(meta_for_Hf_test)\n",
        "\n",
        "# Ensure y_test is a numpy array or pandas Series\n",
        "if isinstance(y_test, pd.Series):\n",
        "    y_test_np = y_test.values\n",
        "else:\n",
        "    y_test_np = y_test\n",
        "\n",
        "# Calculate the accuracy score of the incremental blender\n",
        "acc_inc = accuracy_score(y_test_np, y_pred_test_inc)\n",
        "\n",
        "# Calculate the F1 score of the incremental blender\n",
        "f1_inc = f1_score(y_test_np, y_pred_test_inc)\n",
        "\n",
        "# Print the calculated incremental test accuracy and F1 score\n",
        "print(\"SHIA Incremental - Test Accuracy:\", acc_inc, \"F1:\", f1_inc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIA Incremental - Test Accuracy: 0.7388059701492538 F1: 0.6728971962616822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2496b787"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the trained models and log metrics and artifacts using MLflow as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dd412f7",
        "outputId": "5478eb1c-be62-4bbe-ea6a-a1d9a0921748"
      },
      "source": [
        "# ----------------------------\n",
        "# 12) Guardar modelos y registrar en MLflow (local)\n",
        "# ----------------------------\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save base models\n",
        "for name, mdl in models_trained.items():\n",
        "    p = models_dir / f\"{name}.joblib\"\n",
        "    joblib.dump(mdl, p)\n",
        "\n",
        "# Save meta models & final\n",
        "joblib.dump(H1, models_dir / \"H1.joblib\")\n",
        "joblib.dump(H2, models_dir / \"H2.joblib\")\n",
        "joblib.dump(final_blender, models_dir / \"Hf_initial.joblib\")\n",
        "joblib.dump(Hf_new, models_dir / \"Hf_new.joblib\")\n",
        "# joblib.dump(Hf_final, models_dir / \"Hf_final.joblib\") # Hf_final was not defined in previous steps\n",
        "print(\"Modelos guardados en\", models_dir.resolve())\n",
        "\n",
        "# MLflow logging (local store)\n",
        "mlflow.set_experiment(\"SHIA_Enterprise_Hybrid\")\n",
        "with mlflow.start_run(run_name=\"shia_hybrid_run\"):\n",
        "    mlflow.log_metric(\"shia_initial_acc\", float(acc))\n",
        "    mlflow.log_metric(\"shia_initial_f1\", float(f1))\n",
        "    mlflow.log_metric(\"shia_incremental_acc\", float(acc_inc))\n",
        "    mlflow.log_metric(\"shia_incremental_f1\", float(f1_inc))\n",
        "    # log artifacts (models)\n",
        "    for file in models_dir.glob(\"*.joblib\"):\n",
        "        mlflow.log_artifact(str(file), artifact_path=\"models\")\n",
        "print(\"Registro MLflow completado. Revisa la carpeta ./mlruns por detalles.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos guardados en /content/models\n",
            "Registro MLflow completado. Revisa la carpeta ./mlruns por detalles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d2ce5b4"
      },
      "source": [
        "**Reasoning**:\n",
        "Close the Dask client and cluster, and stop the Spark session to release resources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "773194e4",
        "outputId": "603eafaf-7a94-4af5-b230-0f6737f5c6e2"
      },
      "source": [
        "# ----------------------------\n",
        "# 13) Cleanup recursos\n",
        "# ----------------------------\n",
        "client.close()\n",
        "cluster.close()\n",
        "spark.stop()\n",
        "print(\"Recursos liberados. Fin del pipeline SHIA.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Remove client Client-d4edeb42-b065-11f0-80ea-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:42064; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-d4edeb42-b065-11f0-80ea-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-d4edeb42-b065-11f0-80ea-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1761261351.5833685') (0, 1, 2, 3)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:45777'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:32995'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:44823'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36443'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:42048; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:42042; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:45019 name: 0 (stimulus_id='handle-worker-cleanup-1761261351.6634307')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:37567 name: 1 (stimulus_id='handle-worker-cleanup-1761261351.6721904')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:42026; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:46855 name: 2 (stimulus_id='handle-worker-cleanup-1761261351.708866')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:42024; closing.\n",
            "INFO:distributed.batched:Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:35843 remote=tcp://127.0.0.1:42026>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/distributed/batched.py\", line 115, in _background_send\n",
            "    nbytes = yield coro\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/gen.py\", line 769, in run\n",
            "    value = future.result()\n",
            "            ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/distributed/comm/tcp.py\", line 263, in write\n",
            "    raise CommClosedError()\n",
            "distributed.comm.core.CommClosedError\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:44493 name: 3 (stimulus_id='handle-worker-cleanup-1761261351.745024')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:45777' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:36443' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:44823' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:32995' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recursos liberados. Fin del pipeline SHIA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b4bf44f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "## Summary of Solving Process\n",
        "\n",
        "This task involved implementing the initial steps of a SHIA (Stacking Hybrid Incremental Adaptation) pipeline, including data loading, preprocessing, distributed base model training using Dask, generating meta-features, training meta-learners (H1 and H2), training the initial final blender, simulating incremental data, training an incremental blender, evaluating both blenders, saving models, and logging results with MLflow.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset was successfully downloaded and loaded into Pandas, Spark, and Dask DataFrames.\n",
        "*   Basic preprocessing (handling missing values and encoding) was applied to the data using Spark.\n",
        "*   The data was split into training, validation, and test sets.\n",
        "*   Base models (Random Forest, Gradient Boosting, SVC, Logistic Regression) were trained in parallel using Dask.\n",
        "*   Out-of-fold predictions were generated from base models on the training set to create meta-features for training meta-learners.\n",
        "*   Meta-features were generated for the validation and test sets using the fully trained base models.\n",
        "*   Two meta-learners (H1 and H2) were trained using the OOF meta-features from the training set, splitting the meta-features at the midpoint (using the first 2 for H1 and the remaining for H2).\n",
        "*   The initial final blender (`StackingClassifier`) was trained using the predicted probabilities of H1 and H2 on the validation set as input features.\n",
        "*   Simulated incremental data was generated by adding noise to a subset of the training data, and meta-features were generated for this new data using the base models.\n",
        "*   An incremental blender was trained on the combined original validation data and the simulated new data.\n",
        "*   Evaluation on the test set showed the incremental blender achieved a higher F1 score (0.673) compared to the initial blender (0.627).\n",
        "*   All trained models were saved locally, and evaluation metrics (accuracy and F1 for both initial and incremental blenders) and model artifacts were logged using MLflow.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The incremental training step demonstrated a positive impact on the F1 score, suggesting the SHIA approach could be beneficial for adapting to new data streams.\n",
        "*   Further steps would involve evaluating the performance of the combined base models and meta-learners on the incremental data and potentially implementing a more sophisticated strategy for updating the meta-learners or base models in an incremental setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "1039c1da",
        "outputId": "11e51754-301a-46d6-a27d-3f9e48fa86d3"
      },
      "source": [
        "# ----------------------------\n",
        "# 1) Configuración inicial y carga de datos\n",
        "# ----------------------------\n",
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import mlflow\n",
        "\n",
        "# Repro\n",
        "RND = 42\n",
        "np.random.seed(RND)\n",
        "\n",
        "# Descargar CSV localmente (Spark no lee https directamente)\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "local_path = \"/tmp/titanic.csv\"\n",
        "os.makedirs(\"/tmp\", exist_ok=True)\n",
        "\n",
        "if not os.path.exists(local_path):\n",
        "    print(\"Descargando dataset:\", url)\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    with open(local_path, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "    print(\"Descarga completa ->\", local_path)\n",
        "else:\n",
        "    print(\"Archivo ya existe:\", local_path)\n",
        "\n",
        "# Cargar con Pandas (inspección rápida)\n",
        "pdf = pd.read_csv(local_path)\n",
        "print(\"Pandas df shape:\", pdf.shape)\n",
        "display(pdf.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo ya existe: /tmp/titanic.csv\n",
            "Pandas df shape: (891, 12)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f93be1a5-436b-4cb3-a1c6-5c82363b68ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f93be1a5-436b-4cb3-a1c6-5c82363b68ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f93be1a5-436b-4cb3-a1c6-5c82363b68ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f93be1a5-436b-4cb3-a1c6-5c82363b68ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7430e5f2-6963-4377-a87b-4fbcee27e220\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7430e5f2-6963-4377-a87b-4fbcee27e220')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7430e5f2-6963-4377-a87b-4fbcee27e220 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pdf\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"PassengerId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Braund, Mr. Owen Harris\",\n          \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.32666399786453,\n        \"min\": 22.0,\n        \"max\": 38.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          22.0,\n          38.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ticket\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A/5 21171\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36.77633594396447,\n        \"min\": 7.25,\n        \"max\": 71.2833,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          7.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cabin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"C85\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14bdb2bc",
        "outputId": "33469903-dca3-413b-f389-d9acb9ac0738"
      },
      "source": [
        "# ----------------------------\n",
        "# 3) Inicializar Dask\n",
        "# ----------------------------\n",
        "from dask.distributed import Client, LocalCluster, default_client\n",
        "\n",
        "# Inicializar Dask LocalCluster\n",
        "# Allow Dask to find an available port\n",
        "\n",
        "# Close any existing client and cluster\n",
        "try:\n",
        "    client = default_client()\n",
        "    client.close()\n",
        "    cluster = client.cluster\n",
        "    cluster.close()\n",
        "except:\n",
        "    pass # No existing client or cluster\n",
        "\n",
        "\n",
        "cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit=\"2GB\")\n",
        "client = Client(cluster)\n",
        "print(\"Dask dashboard:\", client.dashboard_link)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 34537 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:39643\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:34537/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:39273'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41453'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:40913'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:32867'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:35447 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35447\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:53016\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:37041 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37041\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:53020\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:41895 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41895\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:53036\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:41563 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41563\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:53046\n",
            "INFO:distributed.scheduler:Receive client connection: Client-82700d5d-b066-11f0-80ea-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:53056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask dashboard: http://127.0.0.1:34537/status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "885738bd",
        "outputId": "65860f1b-6567-476e-c182-43f724f71715"
      },
      "source": [
        "# ----------------------------\n",
        "# 2) Inicializar Spark\n",
        "# ----------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicializar Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SHIA_Enterprise_Hybrid\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "print(\"Spark versión:\", spark.version)\n",
        "\n",
        "# Convertir Pandas -> Spark DataFrame (para usar Spark en preprocesamiento si es necesario)\n",
        "spark_df = spark.createDataFrame(pdf)\n",
        "print(\"Spark DF creado:\")\n",
        "spark_df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark versión: 3.5.1\n",
            "Spark DF creado:\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|  NaN|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|  NaN|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}